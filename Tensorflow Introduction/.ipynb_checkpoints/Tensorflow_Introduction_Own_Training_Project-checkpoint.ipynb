{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af9325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cc438",
   "metadata": {},
   "source": [
    "Load and pre-process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a34adb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart = pd.read_csv(\"./Dataset/HeartDataset.csv\")\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2e02da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df_heart.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c65a32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333    1   3  0.481132  0.244292    1        0  0.603053      0   \n",
       "1  0.166667    1   2  0.339623  0.283105    0        1  0.885496      0   \n",
       "2  0.250000    0   1  0.339623  0.178082    0        0  0.770992      0   \n",
       "3  0.562500    1   1  0.245283  0.251142    0        1  0.816794      0   \n",
       "4  0.583333    0   0  0.245283  0.520548    0        1  0.702290      1   \n",
       "\n",
       "   oldpeak  slope  ca  thal  target  \n",
       "0      2.3      0   0     1       1  \n",
       "1      3.5      0   0     2       1  \n",
       "2      1.4      2   0     2       1  \n",
       "3      0.8      2   0     2       1  \n",
       "4      0.6      2   0     2       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the Dataset using sklearn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columnsToNormalize = ['age', 'trestbps', 'chol', 'thalach']\n",
    "for column in columnsToNormalize:\n",
    "    df_normalized[column] = MinMaxScaler().fit_transform(np.array(df_normalized[column]).reshape(-1,1))\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d5b5530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract the target column as our true label vector\n",
    "train_Y = np.array(df_heart['target'])\n",
    "df_normalized.drop(['target'], axis=1, inplace=True)\n",
    "train_Y = train_Y.astype('float32')\n",
    "train_Y = train_Y.reshape(299,1)\n",
    "# print(train_Y)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f9a8532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 13)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(df_normalized.values)\n",
    "train_X = train_X.astype('float32')\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443ea27",
   "metadata": {},
   "source": [
    "Now out Dataset and X,Y train variables are ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8618148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initialization for our parameters W,b\n",
    "\n",
    "def initialize_parameters():\n",
    "    # My model will have tree layers of 13 (Features layer), 5 Units, 3 Units and the Output Layer with 1 Unit\n",
    "    # I will use Tensorflow Keras initializers to do this\n",
    "    \n",
    "    # And... I'll use the He initialization because i have Relu activations in my model\n",
    "    initializerHe = tf.keras.initializers.HeNormal(seed=1)\n",
    "    \n",
    "    # And a Glotot Normal to the output layer (Sigmoid)\n",
    "    initializerGn = tf.keras.initializers.GlorotNormal(seed=1)\n",
    "    \n",
    "    W1 = tf.Variable(initializerHe(shape=(5, 13)))\n",
    "    b1 = tf.Variable(initializerHe(shape=(5, 1)))\n",
    "    W2 = tf.Variable(initializerHe(shape=(3, 5)))\n",
    "    b2 = tf.Variable(initializerHe(shape=(3, 1)))\n",
    "    W3 = tf.Variable(initializerGn(shape=(1, 3)))\n",
    "    b3 = tf.Variable(initializerGn(shape=(1, 1)))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "453f06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32419413",
   "metadata": {},
   "source": [
    "Now, create the Fordward Propagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ff5bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve the parameters\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # Activations...\n",
    "    \n",
    "    Z1 = tf.math.add(tf.linalg.matmul(W1,X), b1)\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.math.add(tf.linalg.matmul(W2,A1), b2)\n",
    "    A2 = tf.keras.activations.relu(Z2)\n",
    "    Z3 = tf.math.add(tf.linalg.matmul(W3,A2), b3)\n",
    "    A3 = tf.keras.activations.sigmoid(Z3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49bfd33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.3134966  1.4538339  0.66606253 1.2266047  0.36904702 0.49308527\n",
      "  0.26470193 1.6985686  1.7177962  1.2588872  0.25732386 2.042914\n",
      "  1.394236   1.1741472  2.0191913  0.833499   2.1805544  0.7762161\n",
      "  0.17727886 0.14398205 0.14718632 2.028232   1.1843702  0.905777\n",
      "  1.8931507  0.14398205 1.1476511  1.9337988  0.41409823 1.0691688\n",
      "  0.6527782  0.47666803 1.8482288  0.6049826  0.7880223  0.3779548\n",
      "  2.113419   1.1220694  1.4419184  1.4528129  0.14398205 1.3120744\n",
      "  0.14398205 0.21279314 2.5371351  1.6561298  2.4589772  2.3805795\n",
      "  2.5868368  0.78828627 1.8343743  0.31602162 0.14398205 1.5812181\n",
      "  2.064909   0.14398205 1.1335421  1.1783781  3.2387419  0.14398205\n",
      "  0.82974803 1.5920186  2.8863678  1.5407181  2.208911   0.14398205\n",
      "  1.0749685  0.88669646 1.8382704  0.9483251  1.5614028  1.0225141\n",
      "  2.0016243  0.8133675  1.8050791  0.23242772 0.73563546 1.4127707\n",
      "  1.7031578  1.2362827  2.5650034  1.8225487  1.1150486  1.6740694\n",
      "  0.22097908 0.5886289  0.35968477 1.6069825  0.8138626  0.14398205\n",
      "  0.28512597 0.65507174 0.14398205 0.15037623 1.2439038  0.50208104\n",
      "  0.14398205 0.14398205 0.2180388  0.14398205 0.49082235 1.469883\n",
      "  0.14398205 1.1304843  2.3780866  0.5967895  1.1425228  0.21749276\n",
      "  0.8750248  0.8750269  0.62159336 0.8622273  1.6868451  1.0453123\n",
      "  1.6719916  2.369339   0.85778844 1.2571535  1.7203243  0.36413527\n",
      "  0.14398205 1.0080585  2.1095293  2.2171617  2.428001   1.3605622\n",
      "  1.0475409  0.9391057  1.7389183  0.14398205 1.0187862  1.1985226\n",
      "  1.8584027  1.857666   1.6701171  0.95846546 1.9140475  1.4883134\n",
      "  0.14398205 0.14398205 1.8199372  0.14398205 1.9651172  0.14398205\n",
      "  0.8258221  1.4122111  0.65583134 2.138361   2.4846768  2.4262471\n",
      "  0.14398205 0.14398205 1.8671703  0.55482733 1.9068503  0.14398205\n",
      "  2.4587226  1.9318618  0.14398205 1.5079064  0.9996598  0.7623207\n",
      "  1.8835082  0.14398205 0.14398205 0.14398205 0.14398205 0.31025994\n",
      "  0.3724369  0.31322733 0.17832364 0.14398205 0.14398205 0.14398205\n",
      "  0.14398205 2.2240367  0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  0.7625103  0.14398205 0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  0.41252527 1.0235921  0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  0.50256115 0.35096377 0.8923991  0.14398205 0.14398205 0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.2545416  0.14398205 0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.14398205 0.45647246 0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.6026648  0.5269302  0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.14398205 2.1975455  0.33204085\n",
      "  2.445621   0.14398205 0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.79255974 0.14398205 0.25445414\n",
      "  0.14398205 0.14398205 0.14398205 0.3060245  0.14398205 0.14398205\n",
      "  0.41614702 0.14398205 0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  2.7783399  0.14398205 0.14398205 0.14398205 0.14398205 0.6893243\n",
      "  0.14398205 0.14398205 0.14398205 0.14398205 0.14398205 0.14398205\n",
      "  0.14398205 0.14398205 0.14398205 0.14398205 0.40701076 0.27511173\n",
      "  0.14398205 0.14398205 0.14398205 0.14398205 0.14398205 1.2849472\n",
      "  0.14398205 0.14398205 0.3202899  0.43284488 0.70421284 1.0261939\n",
      "  0.14398205 0.14398205 0.3024217  0.5474524  0.14398205 0.14398205\n",
      "  0.14398205 0.6628445  0.14398205 1.0891855  0.8603807  0.14398205\n",
      "  0.43621096 0.14398205 0.14398205 1.7932966  0.14398205]], shape=(1, 299), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Fordward Propagation test\n",
    "print(forward_propagation(train_X.transpose(), parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d6a33",
   "metadata": {},
   "source": [
    "Let's create the Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d3c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(predicted_Y, true_Y):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(true_Y, predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c518926b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.8070068>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cost testing\n",
    "pred = tf.constant([[ 2.4048107,   5.0334096 ],\n",
    "             [-0.7921977,  -4.1523376 ],\n",
    "             [ 0.9447198,  -0.46802214],\n",
    "             [ 1.158121,    3.9810789 ],\n",
    "             [ 4.768706,    2.3220146 ],\n",
    "             [ 6.1481323,   3.909829  ]])\n",
    "true = tf.constant([[ 1,   0 ],\n",
    "             [1,  1 ],\n",
    "             [ 0,  0],\n",
    "             [ 0,    0 ],\n",
    "             [ 1,    1 ],\n",
    "             [ 1,   0  ]])\n",
    "compute_cost(pred, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5b070",
   "metadata": {},
   "source": [
    "Now, let's create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a586907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, learning_rate = 0.0001, num_epochs = 1500, print_cost = True):\n",
    "    \n",
    "    # Save all the costs around the training\n",
    "    costs = []\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # I want to use the Adam optimizer in my model, in my experience, it's very efficient\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Do the fordward propagation\n",
    "            Z3 = forward_propagation(tf.transpose(X_train), parameters)\n",
    "\n",
    "            # Compute the cost function\n",
    "            cost = compute_cost(tf.transpose(Z3), tf.transpose(Y_train))\n",
    "\n",
    "        trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "        grads = tape.gradient(cost, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d612e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.582094\n",
      "Cost after epoch 10: 0.575251\n",
      "Cost after epoch 20: 0.570094\n",
      "Cost after epoch 30: 0.565550\n",
      "Cost after epoch 40: 0.561155\n",
      "Cost after epoch 50: 0.557430\n",
      "Cost after epoch 60: 0.553819\n",
      "Cost after epoch 70: 0.550531\n",
      "Cost after epoch 80: 0.547449\n",
      "Cost after epoch 90: 0.544545\n",
      "Cost after epoch 100: 0.541578\n",
      "Cost after epoch 110: 0.538654\n",
      "Cost after epoch 120: 0.536022\n",
      "Cost after epoch 130: 0.533390\n",
      "Cost after epoch 140: 0.530681\n",
      "Cost after epoch 150: 0.527761\n",
      "Cost after epoch 160: 0.524590\n",
      "Cost after epoch 170: 0.520998\n",
      "Cost after epoch 180: 0.517486\n",
      "Cost after epoch 190: 0.514790\n",
      "Cost after epoch 200: 0.512196\n",
      "Cost after epoch 210: 0.509649\n",
      "Cost after epoch 220: 0.507175\n",
      "Cost after epoch 230: 0.504746\n",
      "Cost after epoch 240: 0.502272\n",
      "Cost after epoch 250: 0.499729\n",
      "Cost after epoch 260: 0.497205\n",
      "Cost after epoch 270: 0.494687\n",
      "Cost after epoch 280: 0.492132\n",
      "Cost after epoch 290: 0.489649\n",
      "Cost after epoch 300: 0.487304\n",
      "Cost after epoch 310: 0.485029\n",
      "Cost after epoch 320: 0.482771\n",
      "Cost after epoch 330: 0.480482\n",
      "Cost after epoch 340: 0.478183\n",
      "Cost after epoch 350: 0.475897\n",
      "Cost after epoch 360: 0.473656\n",
      "Cost after epoch 370: 0.471530\n",
      "Cost after epoch 380: 0.469442\n",
      "Cost after epoch 390: 0.467412\n",
      "Cost after epoch 400: 0.465455\n",
      "Cost after epoch 410: 0.463551\n",
      "Cost after epoch 420: 0.461731\n",
      "Cost after epoch 430: 0.459938\n",
      "Cost after epoch 440: 0.458171\n",
      "Cost after epoch 450: 0.456382\n",
      "Cost after epoch 460: 0.454646\n",
      "Cost after epoch 470: 0.452936\n",
      "Cost after epoch 480: 0.451237\n",
      "Cost after epoch 490: 0.449547\n",
      "Cost after epoch 500: 0.447888\n",
      "Cost after epoch 510: 0.446257\n",
      "Cost after epoch 520: 0.444646\n",
      "Cost after epoch 530: 0.443061\n",
      "Cost after epoch 540: 0.441449\n",
      "Cost after epoch 550: 0.439807\n",
      "Cost after epoch 560: 0.438209\n",
      "Cost after epoch 570: 0.436666\n",
      "Cost after epoch 580: 0.435143\n",
      "Cost after epoch 590: 0.433638\n",
      "Cost after epoch 600: 0.432131\n",
      "Cost after epoch 610: 0.430655\n",
      "Cost after epoch 620: 0.429167\n",
      "Cost after epoch 630: 0.427642\n",
      "Cost after epoch 640: 0.426171\n",
      "Cost after epoch 650: 0.424662\n",
      "Cost after epoch 660: 0.423144\n",
      "Cost after epoch 670: 0.421690\n",
      "Cost after epoch 680: 0.420235\n",
      "Cost after epoch 690: 0.418817\n",
      "Cost after epoch 700: 0.417437\n",
      "Cost after epoch 710: 0.416076\n",
      "Cost after epoch 720: 0.414727\n",
      "Cost after epoch 730: 0.413390\n",
      "Cost after epoch 740: 0.412077\n",
      "Cost after epoch 750: 0.410785\n",
      "Cost after epoch 760: 0.409515\n",
      "Cost after epoch 770: 0.408260\n",
      "Cost after epoch 780: 0.406968\n",
      "Cost after epoch 790: 0.405517\n",
      "Cost after epoch 800: 0.403929\n",
      "Cost after epoch 810: 0.402435\n",
      "Cost after epoch 820: 0.401020\n",
      "Cost after epoch 830: 0.399659\n",
      "Cost after epoch 840: 0.398312\n",
      "Cost after epoch 850: 0.396955\n",
      "Cost after epoch 860: 0.395537\n",
      "Cost after epoch 870: 0.393795\n",
      "Cost after epoch 880: 0.391912\n",
      "Cost after epoch 890: 0.390225\n",
      "Cost after epoch 900: 0.388627\n",
      "Cost after epoch 910: 0.387095\n",
      "Cost after epoch 920: 0.385673\n",
      "Cost after epoch 930: 0.384258\n",
      "Cost after epoch 940: 0.382859\n",
      "Cost after epoch 950: 0.381428\n",
      "Cost after epoch 960: 0.380112\n",
      "Cost after epoch 970: 0.378749\n",
      "Cost after epoch 980: 0.377457\n",
      "Cost after epoch 990: 0.376196\n",
      "Cost after epoch 1000: 0.374964\n",
      "Cost after epoch 1010: 0.373736\n",
      "Cost after epoch 1020: 0.372554\n",
      "Cost after epoch 1030: 0.371409\n",
      "Cost after epoch 1040: 0.370287\n",
      "Cost after epoch 1050: 0.369190\n",
      "Cost after epoch 1060: 0.368120\n",
      "Cost after epoch 1070: 0.367089\n",
      "Cost after epoch 1080: 0.366053\n",
      "Cost after epoch 1090: 0.364955\n",
      "Cost after epoch 1100: 0.363889\n",
      "Cost after epoch 1110: 0.362886\n",
      "Cost after epoch 1120: 0.361887\n",
      "Cost after epoch 1130: 0.360870\n",
      "Cost after epoch 1140: 0.359832\n",
      "Cost after epoch 1150: 0.358786\n",
      "Cost after epoch 1160: 0.357776\n",
      "Cost after epoch 1170: 0.356779\n",
      "Cost after epoch 1180: 0.355821\n",
      "Cost after epoch 1190: 0.354866\n",
      "Cost after epoch 1200: 0.353941\n",
      "Cost after epoch 1210: 0.352919\n",
      "Cost after epoch 1220: 0.351974\n",
      "Cost after epoch 1230: 0.351054\n",
      "Cost after epoch 1240: 0.350143\n",
      "Cost after epoch 1250: 0.349267\n",
      "Cost after epoch 1260: 0.348404\n",
      "Cost after epoch 1270: 0.347549\n",
      "Cost after epoch 1280: 0.346718\n",
      "Cost after epoch 1290: 0.345906\n",
      "Cost after epoch 1300: 0.345053\n",
      "Cost after epoch 1310: 0.344235\n",
      "Cost after epoch 1320: 0.343412\n",
      "Cost after epoch 1330: 0.342595\n",
      "Cost after epoch 1340: 0.341788\n",
      "Cost after epoch 1350: 0.340934\n",
      "Cost after epoch 1360: 0.340095\n",
      "Cost after epoch 1370: 0.339150\n",
      "Cost after epoch 1380: 0.338214\n",
      "Cost after epoch 1390: 0.337243\n",
      "Cost after epoch 1400: 0.336323\n",
      "Cost after epoch 1410: 0.335466\n",
      "Cost after epoch 1420: 0.334626\n",
      "Cost after epoch 1430: 0.333834\n",
      "Cost after epoch 1440: 0.332984\n",
      "Cost after epoch 1450: 0.332237\n",
      "Cost after epoch 1460: 0.331402\n",
      "Cost after epoch 1470: 0.330626\n",
      "Cost after epoch 1480: 0.329844\n",
      "Cost after epoch 1490: 0.329051\n",
      "Cost after epoch 1500: 0.328216\n",
      "Cost after epoch 1510: 0.327391\n",
      "Cost after epoch 1520: 0.326606\n",
      "Cost after epoch 1530: 0.325781\n",
      "Cost after epoch 1540: 0.325006\n",
      "Cost after epoch 1550: 0.324267\n",
      "Cost after epoch 1560: 0.323493\n",
      "Cost after epoch 1570: 0.322748\n",
      "Cost after epoch 1580: 0.321983\n",
      "Cost after epoch 1590: 0.321243\n",
      "Cost after epoch 1600: 0.320516\n",
      "Cost after epoch 1610: 0.319827\n",
      "Cost after epoch 1620: 0.319284\n",
      "Cost after epoch 1630: 0.318492\n",
      "Cost after epoch 1640: 0.317829\n",
      "Cost after epoch 1650: 0.317218\n",
      "Cost after epoch 1660: 0.316528\n",
      "Cost after epoch 1670: 0.315913\n",
      "Cost after epoch 1680: 0.315281\n",
      "Cost after epoch 1690: 0.314656\n",
      "Cost after epoch 1700: 0.314052\n",
      "Cost after epoch 1710: 0.313434\n",
      "Cost after epoch 1720: 0.312863\n",
      "Cost after epoch 1730: 0.312252\n",
      "Cost after epoch 1740: 0.311645\n",
      "Cost after epoch 1750: 0.311078\n",
      "Cost after epoch 1760: 0.310467\n",
      "Cost after epoch 1770: 0.309877\n",
      "Cost after epoch 1780: 0.309266\n",
      "Cost after epoch 1790: 0.308647\n",
      "Cost after epoch 1800: 0.308043\n",
      "Cost after epoch 1810: 0.307422\n",
      "Cost after epoch 1820: 0.306813\n",
      "Cost after epoch 1830: 0.306208\n",
      "Cost after epoch 1840: 0.305606\n",
      "Cost after epoch 1850: 0.305014\n",
      "Cost after epoch 1860: 0.304402\n",
      "Cost after epoch 1870: 0.303874\n",
      "Cost after epoch 1880: 0.303264\n",
      "Cost after epoch 1890: 0.302785\n",
      "Cost after epoch 1900: 0.302239\n",
      "Cost after epoch 1910: 0.301646\n",
      "Cost after epoch 1920: 0.301187\n",
      "Cost after epoch 1930: 0.300618\n",
      "Cost after epoch 1940: 0.300089\n",
      "Cost after epoch 1950: 0.299596\n",
      "Cost after epoch 1960: 0.299051\n",
      "Cost after epoch 1970: 0.298535\n",
      "Cost after epoch 1980: 0.298002\n",
      "Cost after epoch 1990: 0.297490\n",
      "Cost after epoch 2000: 0.296983\n",
      "Cost after epoch 2010: 0.296492\n",
      "Cost after epoch 2020: 0.295964\n",
      "Cost after epoch 2030: 0.295477\n",
      "Cost after epoch 2040: 0.294988\n",
      "Cost after epoch 2050: 0.294504\n",
      "Cost after epoch 2060: 0.294051\n",
      "Cost after epoch 2070: 0.293554\n",
      "Cost after epoch 2080: 0.293088\n",
      "Cost after epoch 2090: 0.292606\n",
      "Cost after epoch 2100: 0.292129\n",
      "Cost after epoch 2110: 0.291654\n",
      "Cost after epoch 2120: 0.291149\n",
      "Cost after epoch 2130: 0.290569\n",
      "Cost after epoch 2140: 0.290012\n",
      "Cost after epoch 2150: 0.289483\n",
      "Cost after epoch 2160: 0.288981\n",
      "Cost after epoch 2170: 0.288485\n",
      "Cost after epoch 2180: 0.288017\n",
      "Cost after epoch 2190: 0.287561\n",
      "Cost after epoch 2200: 0.287075\n",
      "Cost after epoch 2210: 0.286591\n",
      "Cost after epoch 2220: 0.286114\n",
      "Cost after epoch 2230: 0.285633\n",
      "Cost after epoch 2240: 0.285142\n",
      "Cost after epoch 2250: 0.284494\n",
      "Cost after epoch 2260: 0.283739\n",
      "Cost after epoch 2270: 0.283142\n",
      "Cost after epoch 2280: 0.282557\n",
      "Cost after epoch 2290: 0.282027\n",
      "Cost after epoch 2300: 0.281490\n",
      "Cost after epoch 2310: 0.280952\n",
      "Cost after epoch 2320: 0.280437\n",
      "Cost after epoch 2330: 0.279938\n",
      "Cost after epoch 2340: 0.279491\n",
      "Cost after epoch 2350: 0.279000\n",
      "Cost after epoch 2360: 0.278590\n",
      "Cost after epoch 2370: 0.278285\n",
      "Cost after epoch 2380: 0.277770\n",
      "Cost after epoch 2390: 0.277248\n",
      "Cost after epoch 2400: 0.276715\n",
      "Cost after epoch 2410: 0.276210\n",
      "Cost after epoch 2420: 0.275713\n",
      "Cost after epoch 2430: 0.275239\n",
      "Cost after epoch 2440: 0.274773\n",
      "Cost after epoch 2450: 0.274298\n",
      "Cost after epoch 2460: 0.273819\n",
      "Cost after epoch 2470: 0.273357\n",
      "Cost after epoch 2480: 0.272940\n",
      "Cost after epoch 2490: 0.272553\n",
      "Cost after epoch 2500: 0.272102\n",
      "Cost after epoch 2510: 0.271710\n",
      "Cost after epoch 2520: 0.271392\n",
      "Cost after epoch 2530: 0.271024\n",
      "Cost after epoch 2540: 0.270550\n",
      "Cost after epoch 2550: 0.270191\n",
      "Cost after epoch 2560: 0.269856\n",
      "Cost after epoch 2570: 0.269514\n",
      "Cost after epoch 2580: 0.269164\n",
      "Cost after epoch 2590: 0.268823\n",
      "Cost after epoch 2600: 0.268485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2610: 0.268083\n",
      "Cost after epoch 2620: 0.267565\n",
      "Cost after epoch 2630: 0.267004\n",
      "Cost after epoch 2640: 0.266518\n",
      "Cost after epoch 2650: 0.266193\n",
      "Cost after epoch 2660: 0.265619\n",
      "Cost after epoch 2670: 0.265223\n",
      "Cost after epoch 2680: 0.264857\n",
      "Cost after epoch 2690: 0.264395\n",
      "Cost after epoch 2700: 0.264080\n",
      "Cost after epoch 2710: 0.263659\n",
      "Cost after epoch 2720: 0.263379\n",
      "Cost after epoch 2730: 0.263011\n",
      "Cost after epoch 2740: 0.262717\n",
      "Cost after epoch 2750: 0.262429\n",
      "Cost after epoch 2760: 0.262117\n",
      "Cost after epoch 2770: 0.261841\n",
      "Cost after epoch 2780: 0.261497\n",
      "Cost after epoch 2790: 0.261289\n",
      "Cost after epoch 2800: 0.260971\n",
      "Cost after epoch 2810: 0.260733\n",
      "Cost after epoch 2820: 0.260569\n",
      "Cost after epoch 2830: 0.260257\n",
      "Cost after epoch 2840: 0.259904\n",
      "Cost after epoch 2850: 0.259719\n",
      "Cost after epoch 2860: 0.259361\n",
      "Cost after epoch 2870: 0.259184\n",
      "Cost after epoch 2880: 0.258817\n",
      "Cost after epoch 2890: 0.258525\n",
      "Cost after epoch 2900: 0.258203\n",
      "Cost after epoch 2910: 0.257907\n",
      "Cost after epoch 2920: 0.257692\n",
      "Cost after epoch 2930: 0.257402\n",
      "Cost after epoch 2940: 0.257100\n",
      "Cost after epoch 2950: 0.256945\n",
      "Cost after epoch 2960: 0.256565\n",
      "Cost after epoch 2970: 0.256384\n",
      "Cost after epoch 2980: 0.256017\n",
      "Cost after epoch 2990: 0.255751\n",
      "Cost after epoch 3000: 0.255501\n",
      "Cost after epoch 3010: 0.255303\n",
      "Cost after epoch 3020: 0.255087\n",
      "Cost after epoch 3030: 0.254926\n",
      "Cost after epoch 3040: 0.254458\n",
      "Cost after epoch 3050: 0.254201\n",
      "Cost after epoch 3060: 0.253944\n",
      "Cost after epoch 3070: 0.253684\n",
      "Cost after epoch 3080: 0.253421\n",
      "Cost after epoch 3090: 0.253208\n",
      "Cost after epoch 3100: 0.252887\n",
      "Cost after epoch 3110: 0.252623\n",
      "Cost after epoch 3120: 0.252374\n",
      "Cost after epoch 3130: 0.252096\n",
      "Cost after epoch 3140: 0.251822\n",
      "Cost after epoch 3150: 0.251725\n",
      "Cost after epoch 3160: 0.251446\n",
      "Cost after epoch 3170: 0.251151\n",
      "Cost after epoch 3180: 0.250864\n",
      "Cost after epoch 3190: 0.250657\n",
      "Cost after epoch 3200: 0.250426\n",
      "Cost after epoch 3210: 0.250310\n",
      "Cost after epoch 3220: 0.250076\n",
      "Cost after epoch 3230: 0.249877\n",
      "Cost after epoch 3240: 0.249628\n",
      "Cost after epoch 3250: 0.249387\n",
      "Cost after epoch 3260: 0.249282\n",
      "Cost after epoch 3270: 0.249030\n",
      "Cost after epoch 3280: 0.248799\n",
      "Cost after epoch 3290: 0.248601\n",
      "Cost after epoch 3300: 0.248396\n",
      "Cost after epoch 3310: 0.248212\n",
      "Cost after epoch 3320: 0.248035\n",
      "Cost after epoch 3330: 0.247850\n",
      "Cost after epoch 3340: 0.247679\n",
      "Cost after epoch 3350: 0.247518\n",
      "Cost after epoch 3360: 0.247332\n",
      "Cost after epoch 3370: 0.247143\n",
      "Cost after epoch 3380: 0.246982\n",
      "Cost after epoch 3390: 0.246802\n",
      "Cost after epoch 3400: 0.246678\n",
      "Cost after epoch 3410: 0.246516\n",
      "Cost after epoch 3420: 0.246341\n",
      "Cost after epoch 3430: 0.246206\n",
      "Cost after epoch 3440: 0.246027\n",
      "Cost after epoch 3450: 0.245905\n",
      "Cost after epoch 3460: 0.245852\n",
      "Cost after epoch 3470: 0.245627\n",
      "Cost after epoch 3480: 0.245533\n",
      "Cost after epoch 3490: 0.245453\n",
      "Cost after epoch 3500: 0.245275\n",
      "Cost after epoch 3510: 0.245091\n",
      "Cost after epoch 3520: 0.244996\n",
      "Cost after epoch 3530: 0.244795\n",
      "Cost after epoch 3540: 0.244706\n",
      "Cost after epoch 3550: 0.244595\n",
      "Cost after epoch 3560: 0.244417\n",
      "Cost after epoch 3570: 0.244449\n",
      "Cost after epoch 3580: 0.244326\n",
      "Cost after epoch 3590: 0.244211\n",
      "Cost after epoch 3600: 0.244096\n",
      "Cost after epoch 3610: 0.243855\n",
      "Cost after epoch 3620: 0.243771\n",
      "Cost after epoch 3630: 0.243634\n",
      "Cost after epoch 3640: 0.243550\n",
      "Cost after epoch 3650: 0.243424\n",
      "Cost after epoch 3660: 0.243332\n",
      "Cost after epoch 3670: 0.243331\n",
      "Cost after epoch 3680: 0.243160\n",
      "Cost after epoch 3690: 0.243029\n",
      "Cost after epoch 3700: 0.242959\n",
      "Cost after epoch 3710: 0.242941\n",
      "Cost after epoch 3720: 0.242820\n",
      "Cost after epoch 3730: 0.242743\n",
      "Cost after epoch 3740: 0.242567\n",
      "Cost after epoch 3750: 0.242494\n",
      "Cost after epoch 3760: 0.242425\n",
      "Cost after epoch 3770: 0.242367\n",
      "Cost after epoch 3780: 0.242249\n",
      "Cost after epoch 3790: 0.242242\n",
      "Cost after epoch 3800: 0.242119\n",
      "Cost after epoch 3810: 0.241984\n",
      "Cost after epoch 3820: 0.241909\n",
      "Cost after epoch 3830: 0.241810\n",
      "Cost after epoch 3840: 0.241736\n",
      "Cost after epoch 3850: 0.241667\n",
      "Cost after epoch 3860: 0.241595\n",
      "Cost after epoch 3870: 0.241537\n",
      "Cost after epoch 3880: 0.241446\n",
      "Cost after epoch 3890: 0.241461\n",
      "Cost after epoch 3900: 0.241308\n",
      "Cost after epoch 3910: 0.241202\n",
      "Cost after epoch 3920: 0.241152\n",
      "Cost after epoch 3930: 0.241054\n",
      "Cost after epoch 3940: 0.240975\n",
      "Cost after epoch 3950: 0.241040\n",
      "Cost after epoch 3960: 0.240886\n",
      "Cost after epoch 3970: 0.240777\n",
      "Cost after epoch 3980: 0.240705\n",
      "Cost after epoch 3990: 0.240687\n",
      "Cost after epoch 4000: 0.240583\n",
      "Cost after epoch 4010: 0.240493\n",
      "Cost after epoch 4020: 0.240446\n",
      "Cost after epoch 4030: 0.240439\n",
      "Cost after epoch 4040: 0.240334\n",
      "Cost after epoch 4050: 0.240308\n",
      "Cost after epoch 4060: 0.240314\n",
      "Cost after epoch 4070: 0.240200\n",
      "Cost after epoch 4080: 0.240115\n",
      "Cost after epoch 4090: 0.240073\n",
      "Cost after epoch 4100: 0.240069\n",
      "Cost after epoch 4110: 0.239957\n",
      "Cost after epoch 4120: 0.239848\n",
      "Cost after epoch 4130: 0.239828\n",
      "Cost after epoch 4140: 0.239784\n",
      "Cost after epoch 4150: 0.239691\n",
      "Cost after epoch 4160: 0.239607\n",
      "Cost after epoch 4170: 0.239656\n",
      "Cost after epoch 4180: 0.239517\n",
      "Cost after epoch 4190: 0.239582\n",
      "Cost after epoch 4200: 0.239431\n",
      "Cost after epoch 4210: 0.239368\n",
      "Cost after epoch 4220: 0.239317\n",
      "Cost after epoch 4230: 0.239254\n",
      "Cost after epoch 4240: 0.239236\n",
      "Cost after epoch 4250: 0.239172\n",
      "Cost after epoch 4260: 0.239106\n",
      "Cost after epoch 4270: 0.239057\n",
      "Cost after epoch 4280: 0.238999\n",
      "Cost after epoch 4290: 0.238951\n",
      "Cost after epoch 4300: 0.238901\n",
      "Cost after epoch 4310: 0.238862\n",
      "Cost after epoch 4320: 0.238837\n",
      "Cost after epoch 4330: 0.238795\n",
      "Cost after epoch 4340: 0.238763\n",
      "Cost after epoch 4350: 0.238680\n",
      "Cost after epoch 4360: 0.238643\n",
      "Cost after epoch 4370: 0.238602\n",
      "Cost after epoch 4380: 0.238533\n",
      "Cost after epoch 4390: 0.238511\n",
      "Cost after epoch 4400: 0.238447\n",
      "Cost after epoch 4410: 0.238380\n",
      "Cost after epoch 4420: 0.238385\n",
      "Cost after epoch 4430: 0.238368\n",
      "Cost after epoch 4440: 0.238358\n",
      "Cost after epoch 4450: 0.238284\n",
      "Cost after epoch 4460: 0.238242\n",
      "Cost after epoch 4470: 0.238142\n",
      "Cost after epoch 4480: 0.238156\n",
      "Cost after epoch 4490: 0.238082\n",
      "Cost after epoch 4500: 0.238054\n",
      "Cost after epoch 4510: 0.238107\n",
      "Cost after epoch 4520: 0.237989\n",
      "Cost after epoch 4530: 0.237942\n",
      "Cost after epoch 4540: 0.237887\n",
      "Cost after epoch 4550: 0.237853\n",
      "Cost after epoch 4560: 0.237818\n",
      "Cost after epoch 4570: 0.237745\n",
      "Cost after epoch 4580: 0.237720\n",
      "Cost after epoch 4590: 0.237667\n",
      "Cost after epoch 4600: 0.237705\n",
      "Cost after epoch 4610: 0.237618\n",
      "Cost after epoch 4620: 0.237633\n",
      "Cost after epoch 4630: 0.237551\n",
      "Cost after epoch 4640: 0.237504\n",
      "Cost after epoch 4650: 0.237481\n",
      "Cost after epoch 4660: 0.237425\n",
      "Cost after epoch 4670: 0.237380\n",
      "Cost after epoch 4680: 0.237369\n",
      "Cost after epoch 4690: 0.237343\n",
      "Cost after epoch 4700: 0.237296\n",
      "Cost after epoch 4710: 0.237301\n",
      "Cost after epoch 4720: 0.237229\n",
      "Cost after epoch 4730: 0.237216\n",
      "Cost after epoch 4740: 0.237147\n",
      "Cost after epoch 4750: 0.237124\n",
      "Cost after epoch 4760: 0.237083\n",
      "Cost after epoch 4770: 0.237164\n",
      "Cost after epoch 4780: 0.237059\n",
      "Cost after epoch 4790: 0.237017\n",
      "Cost after epoch 4800: 0.236953\n",
      "Cost after epoch 4810: 0.236917\n",
      "Cost after epoch 4820: 0.236904\n",
      "Cost after epoch 4830: 0.236870\n",
      "Cost after epoch 4840: 0.236900\n",
      "Cost after epoch 4850: 0.236813\n",
      "Cost after epoch 4860: 0.236777\n",
      "Cost after epoch 4870: 0.236770\n",
      "Cost after epoch 4880: 0.236709\n",
      "Cost after epoch 4890: 0.236742\n",
      "Cost after epoch 4900: 0.236682\n",
      "Cost after epoch 4910: 0.236626\n",
      "Cost after epoch 4920: 0.236605\n",
      "Cost after epoch 4930: 0.236604\n",
      "Cost after epoch 4940: 0.236600\n",
      "Cost after epoch 4950: 0.236514\n",
      "Cost after epoch 4960: 0.236479\n",
      "Cost after epoch 4970: 0.236476\n",
      "Cost after epoch 4980: 0.236440\n",
      "Cost after epoch 4990: 0.236412\n",
      "Cost after epoch 5000: 0.236392\n",
      "Cost after epoch 5010: 0.236325\n",
      "Cost after epoch 5020: 0.236307\n",
      "Cost after epoch 5030: 0.236347\n",
      "Cost after epoch 5040: 0.236267\n",
      "Cost after epoch 5050: 0.236223\n",
      "Cost after epoch 5060: 0.236210\n",
      "Cost after epoch 5070: 0.236161\n",
      "Cost after epoch 5080: 0.236144\n",
      "Cost after epoch 5090: 0.236134\n",
      "Cost after epoch 5100: 0.236101\n",
      "Cost after epoch 5110: 0.236099\n",
      "Cost after epoch 5120: 0.236055\n",
      "Cost after epoch 5130: 0.236146\n",
      "Cost after epoch 5140: 0.236058\n",
      "Cost after epoch 5150: 0.236017\n",
      "Cost after epoch 5160: 0.235943\n",
      "Cost after epoch 5170: 0.235915\n",
      "Cost after epoch 5180: 0.235893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 5190: 0.235920\n",
      "Cost after epoch 5200: 0.235830\n",
      "Cost after epoch 5210: 0.235823\n",
      "Cost after epoch 5220: 0.235841\n",
      "Cost after epoch 5230: 0.235750\n",
      "Cost after epoch 5240: 0.235827\n",
      "Cost after epoch 5250: 0.235836\n",
      "Cost after epoch 5260: 0.235725\n",
      "Cost after epoch 5270: 0.235692\n",
      "Cost after epoch 5280: 0.235652\n",
      "Cost after epoch 5290: 0.235652\n",
      "Cost after epoch 5300: 0.235614\n",
      "Cost after epoch 5310: 0.235557\n",
      "Cost after epoch 5320: 0.235582\n",
      "Cost after epoch 5330: 0.235608\n",
      "Cost after epoch 5340: 0.235535\n",
      "Cost after epoch 5350: 0.235508\n",
      "Cost after epoch 5360: 0.235561\n",
      "Cost after epoch 5370: 0.235486\n",
      "Cost after epoch 5380: 0.235422\n",
      "Cost after epoch 5390: 0.235403\n",
      "Cost after epoch 5400: 0.235378\n",
      "Cost after epoch 5410: 0.235364\n",
      "Cost after epoch 5420: 0.235317\n",
      "Cost after epoch 5430: 0.235298\n",
      "Cost after epoch 5440: 0.235297\n",
      "Cost after epoch 5450: 0.235288\n",
      "Cost after epoch 5460: 0.235249\n",
      "Cost after epoch 5470: 0.235219\n",
      "Cost after epoch 5480: 0.235187\n",
      "Cost after epoch 5490: 0.235177\n",
      "Cost after epoch 5500: 0.235160\n",
      "Cost after epoch 5510: 0.235137\n",
      "Cost after epoch 5520: 0.235099\n",
      "Cost after epoch 5530: 0.235108\n",
      "Cost after epoch 5540: 0.235079\n",
      "Cost after epoch 5550: 0.235048\n",
      "Cost after epoch 5560: 0.235040\n",
      "Cost after epoch 5570: 0.234997\n",
      "Cost after epoch 5580: 0.235033\n",
      "Cost after epoch 5590: 0.235034\n",
      "Cost after epoch 5600: 0.234975\n",
      "Cost after epoch 5610: 0.234956\n",
      "Cost after epoch 5620: 0.234923\n",
      "Cost after epoch 5630: 0.234893\n",
      "Cost after epoch 5640: 0.234867\n",
      "Cost after epoch 5650: 0.234919\n",
      "Cost after epoch 5660: 0.234837\n",
      "Cost after epoch 5670: 0.234792\n",
      "Cost after epoch 5680: 0.234802\n",
      "Cost after epoch 5690: 0.234808\n",
      "Cost after epoch 5700: 0.234743\n",
      "Cost after epoch 5710: 0.234740\n",
      "Cost after epoch 5720: 0.234699\n",
      "Cost after epoch 5730: 0.234739\n",
      "Cost after epoch 5740: 0.234683\n",
      "Cost after epoch 5750: 0.234654\n",
      "Cost after epoch 5760: 0.234628\n",
      "Cost after epoch 5770: 0.234618\n",
      "Cost after epoch 5780: 0.234611\n",
      "Cost after epoch 5790: 0.234597\n",
      "Cost after epoch 5800: 0.234542\n",
      "Cost after epoch 5810: 0.234540\n",
      "Cost after epoch 5820: 0.234507\n",
      "Cost after epoch 5830: 0.234469\n",
      "Cost after epoch 5840: 0.234470\n",
      "Cost after epoch 5850: 0.234461\n",
      "Cost after epoch 5860: 0.234429\n",
      "Cost after epoch 5870: 0.234419\n",
      "Cost after epoch 5880: 0.234452\n",
      "Cost after epoch 5890: 0.234364\n",
      "Cost after epoch 5900: 0.234355\n",
      "Cost after epoch 5910: 0.234334\n",
      "Cost after epoch 5920: 0.234346\n",
      "Cost after epoch 5930: 0.234340\n",
      "Cost after epoch 5940: 0.234287\n",
      "Cost after epoch 5950: 0.234288\n",
      "Cost after epoch 5960: 0.234264\n",
      "Cost after epoch 5970: 0.234221\n",
      "Cost after epoch 5980: 0.234193\n",
      "Cost after epoch 5990: 0.234167\n",
      "Cost after epoch 6000: 0.234155\n",
      "Cost after epoch 6010: 0.234128\n",
      "Cost after epoch 6020: 0.234123\n",
      "Cost after epoch 6030: 0.234095\n",
      "Cost after epoch 6040: 0.234119\n",
      "Cost after epoch 6050: 0.234066\n",
      "Cost after epoch 6060: 0.234063\n",
      "Cost after epoch 6070: 0.234035\n",
      "Cost after epoch 6080: 0.234040\n",
      "Cost after epoch 6090: 0.234008\n",
      "Cost after epoch 6100: 0.234019\n",
      "Cost after epoch 6110: 0.233985\n",
      "Cost after epoch 6120: 0.233947\n",
      "Cost after epoch 6130: 0.233963\n",
      "Cost after epoch 6140: 0.233971\n",
      "Cost after epoch 6150: 0.233935\n",
      "Cost after epoch 6160: 0.233916\n",
      "Cost after epoch 6170: 0.233983\n",
      "Cost after epoch 6180: 0.233870\n",
      "Cost after epoch 6190: 0.233839\n",
      "Cost after epoch 6200: 0.233814\n",
      "Cost after epoch 6210: 0.233779\n",
      "Cost after epoch 6220: 0.233755\n",
      "Cost after epoch 6230: 0.233795\n",
      "Cost after epoch 6240: 0.233764\n",
      "Cost after epoch 6250: 0.233711\n",
      "Cost after epoch 6260: 0.233717\n",
      "Cost after epoch 6270: 0.233722\n",
      "Cost after epoch 6280: 0.233734\n",
      "Cost after epoch 6290: 0.233664\n",
      "Cost after epoch 6300: 0.233641\n",
      "Cost after epoch 6310: 0.233617\n",
      "Cost after epoch 6320: 0.233595\n",
      "Cost after epoch 6330: 0.233597\n",
      "Cost after epoch 6340: 0.233572\n",
      "Cost after epoch 6350: 0.233539\n",
      "Cost after epoch 6360: 0.233529\n",
      "Cost after epoch 6370: 0.233531\n",
      "Cost after epoch 6380: 0.233480\n",
      "Cost after epoch 6390: 0.233451\n",
      "Cost after epoch 6400: 0.233502\n",
      "Cost after epoch 6410: 0.233469\n",
      "Cost after epoch 6420: 0.233423\n",
      "Cost after epoch 6430: 0.233390\n",
      "Cost after epoch 6440: 0.233360\n",
      "Cost after epoch 6450: 0.233349\n",
      "Cost after epoch 6460: 0.233353\n",
      "Cost after epoch 6470: 0.233299\n",
      "Cost after epoch 6480: 0.233324\n",
      "Cost after epoch 6490: 0.233296\n",
      "Cost after epoch 6500: 0.233285\n",
      "Cost after epoch 6510: 0.233272\n",
      "Cost after epoch 6520: 0.233256\n",
      "Cost after epoch 6530: 0.233258\n",
      "Cost after epoch 6540: 0.233211\n",
      "Cost after epoch 6550: 0.233210\n",
      "Cost after epoch 6560: 0.233197\n",
      "Cost after epoch 6570: 0.233129\n",
      "Cost after epoch 6580: 0.233132\n",
      "Cost after epoch 6590: 0.233179\n",
      "Cost after epoch 6600: 0.233095\n",
      "Cost after epoch 6610: 0.233115\n",
      "Cost after epoch 6620: 0.233071\n",
      "Cost after epoch 6630: 0.233056\n",
      "Cost after epoch 6640: 0.233037\n",
      "Cost after epoch 6650: 0.233017\n",
      "Cost after epoch 6660: 0.232991\n",
      "Cost after epoch 6670: 0.232959\n",
      "Cost after epoch 6680: 0.232977\n",
      "Cost after epoch 6690: 0.232934\n",
      "Cost after epoch 6700: 0.232925\n",
      "Cost after epoch 6710: 0.232901\n",
      "Cost after epoch 6720: 0.232870\n",
      "Cost after epoch 6730: 0.232852\n",
      "Cost after epoch 6740: 0.232861\n",
      "Cost after epoch 6750: 0.232835\n",
      "Cost after epoch 6760: 0.232856\n",
      "Cost after epoch 6770: 0.232798\n",
      "Cost after epoch 6780: 0.232798\n",
      "Cost after epoch 6790: 0.232777\n",
      "Cost after epoch 6800: 0.232742\n",
      "Cost after epoch 6810: 0.232727\n",
      "Cost after epoch 6820: 0.232731\n",
      "Cost after epoch 6830: 0.231692\n",
      "Cost after epoch 6840: 0.230414\n",
      "Cost after epoch 6850: 0.229957\n",
      "Cost after epoch 6860: 0.229748\n",
      "Cost after epoch 6870: 0.229584\n",
      "Cost after epoch 6880: 0.229534\n",
      "Cost after epoch 6890: 0.229430\n",
      "Cost after epoch 6900: 0.229346\n",
      "Cost after epoch 6910: 0.229315\n",
      "Cost after epoch 6920: 0.229247\n",
      "Cost after epoch 6930: 0.229197\n",
      "Cost after epoch 6940: 0.229153\n",
      "Cost after epoch 6950: 0.229122\n",
      "Cost after epoch 6960: 0.229046\n",
      "Cost after epoch 6970: 0.229019\n",
      "Cost after epoch 6980: 0.228963\n",
      "Cost after epoch 6990: 0.228914\n",
      "Cost after epoch 7000: 0.228891\n",
      "Cost after epoch 7010: 0.227839\n",
      "Cost after epoch 7020: 0.227071\n",
      "Cost after epoch 7030: 0.226693\n",
      "Cost after epoch 7040: 0.226442\n",
      "Cost after epoch 7050: 0.226260\n",
      "Cost after epoch 7060: 0.226122\n",
      "Cost after epoch 7070: 0.226026\n",
      "Cost after epoch 7080: 0.225922\n",
      "Cost after epoch 7090: 0.225855\n",
      "Cost after epoch 7100: 0.225788\n",
      "Cost after epoch 7110: 0.225698\n",
      "Cost after epoch 7120: 0.225626\n",
      "Cost after epoch 7130: 0.225603\n",
      "Cost after epoch 7140: 0.225526\n",
      "Cost after epoch 7150: 0.225472\n",
      "Cost after epoch 7160: 0.225402\n",
      "Cost after epoch 7170: 0.225375\n",
      "Cost after epoch 7180: 0.225307\n",
      "Cost after epoch 7190: 0.225302\n",
      "Cost after epoch 7200: 0.225211\n",
      "Cost after epoch 7210: 0.225238\n",
      "Cost after epoch 7220: 0.225135\n",
      "Cost after epoch 7230: 0.225124\n",
      "Cost after epoch 7240: 0.225082\n",
      "Cost after epoch 7250: 0.225012\n",
      "Cost after epoch 7260: 0.224993\n",
      "Cost after epoch 7270: 0.224911\n",
      "Cost after epoch 7280: 0.224898\n",
      "Cost after epoch 7290: 0.224871\n",
      "Cost after epoch 7300: 0.224823\n",
      "Cost after epoch 7310: 0.224763\n",
      "Cost after epoch 7320: 0.224729\n",
      "Cost after epoch 7330: 0.224756\n",
      "Cost after epoch 7340: 0.224694\n",
      "Cost after epoch 7350: 0.224681\n",
      "Cost after epoch 7360: 0.224689\n",
      "Cost after epoch 7370: 0.224558\n",
      "Cost after epoch 7380: 0.224534\n",
      "Cost after epoch 7390: 0.224501\n",
      "Cost after epoch 7400: 0.224517\n",
      "Cost after epoch 7410: 0.224447\n",
      "Cost after epoch 7420: 0.224451\n",
      "Cost after epoch 7430: 0.224441\n",
      "Cost after epoch 7440: 0.224428\n",
      "Cost after epoch 7450: 0.224332\n",
      "Cost after epoch 7460: 0.224286\n",
      "Cost after epoch 7470: 0.224360\n",
      "Cost after epoch 7480: 0.224270\n",
      "Cost after epoch 7490: 0.224253\n",
      "Cost after epoch 7500: 0.224261\n",
      "Cost after epoch 7510: 0.224323\n",
      "Cost after epoch 7520: 0.224197\n",
      "Cost after epoch 7530: 0.224092\n",
      "Cost after epoch 7540: 0.224069\n",
      "Cost after epoch 7550: 0.224184\n",
      "Cost after epoch 7560: 0.224033\n",
      "Cost after epoch 7570: 0.224019\n",
      "Cost after epoch 7580: 0.224081\n",
      "Cost after epoch 7590: 0.223956\n",
      "Cost after epoch 7600: 0.223968\n",
      "Cost after epoch 7610: 0.223930\n",
      "Cost after epoch 7620: 0.223912\n",
      "Cost after epoch 7630: 0.223880\n",
      "Cost after epoch 7640: 0.223818\n",
      "Cost after epoch 7650: 0.223808\n",
      "Cost after epoch 7660: 0.223829\n",
      "Cost after epoch 7670: 0.223767\n",
      "Cost after epoch 7680: 0.223752\n",
      "Cost after epoch 7690: 0.223756\n",
      "Cost after epoch 7700: 0.223741\n",
      "Cost after epoch 7710: 0.223686\n",
      "Cost after epoch 7720: 0.223691\n",
      "Cost after epoch 7730: 0.223631\n",
      "Cost after epoch 7740: 0.223601\n",
      "Cost after epoch 7750: 0.223665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 7760: 0.223658\n",
      "Cost after epoch 7770: 0.223566\n",
      "Cost after epoch 7780: 0.223541\n",
      "Cost after epoch 7790: 0.223589\n",
      "Cost after epoch 7800: 0.223459\n",
      "Cost after epoch 7810: 0.223484\n",
      "Cost after epoch 7820: 0.223533\n",
      "Cost after epoch 7830: 0.223542\n",
      "Cost after epoch 7840: 0.223481\n",
      "Cost after epoch 7850: 0.223380\n",
      "Cost after epoch 7860: 0.223352\n",
      "Cost after epoch 7870: 0.223312\n",
      "Cost after epoch 7880: 0.223292\n",
      "Cost after epoch 7890: 0.223343\n",
      "Cost after epoch 7900: 0.223268\n",
      "Cost after epoch 7910: 0.223259\n",
      "Cost after epoch 7920: 0.223246\n",
      "Cost after epoch 7930: 0.223174\n",
      "Cost after epoch 7940: 0.223173\n",
      "Cost after epoch 7950: 0.223142\n",
      "Cost after epoch 7960: 0.223192\n",
      "Cost after epoch 7970: 0.223242\n",
      "Cost after epoch 7980: 0.223063\n",
      "Cost after epoch 7990: 0.223097\n",
      "Cost after epoch 8000: 0.223014\n",
      "Cost after epoch 8010: 0.223029\n",
      "Cost after epoch 8020: 0.223013\n",
      "Cost after epoch 8030: 0.222969\n",
      "Cost after epoch 8040: 0.223030\n",
      "Cost after epoch 8050: 0.223015\n",
      "Cost after epoch 8060: 0.222920\n",
      "Cost after epoch 8070: 0.222898\n",
      "Cost after epoch 8080: 0.222877\n",
      "Cost after epoch 8090: 0.222949\n",
      "Cost after epoch 8100: 0.222890\n",
      "Cost after epoch 8110: 0.223020\n",
      "Cost after epoch 8120: 0.222830\n",
      "Cost after epoch 8130: 0.222774\n",
      "Cost after epoch 8140: 0.222812\n",
      "Cost after epoch 8150: 0.222857\n",
      "Cost after epoch 8160: 0.222792\n",
      "Cost after epoch 8170: 0.222793\n",
      "Cost after epoch 8180: 0.222763\n",
      "Cost after epoch 8190: 0.222775\n",
      "Cost after epoch 8200: 0.222767\n",
      "Cost after epoch 8210: 0.222635\n",
      "Cost after epoch 8220: 0.222621\n",
      "Cost after epoch 8230: 0.222651\n",
      "Cost after epoch 8240: 0.222625\n",
      "Cost after epoch 8250: 0.222634\n",
      "Cost after epoch 8260: 0.222571\n",
      "Cost after epoch 8270: 0.222516\n",
      "Cost after epoch 8280: 0.222558\n",
      "Cost after epoch 8290: 0.222501\n",
      "Cost after epoch 8300: 0.222454\n",
      "Cost after epoch 8310: 0.222452\n",
      "Cost after epoch 8320: 0.222429\n",
      "Cost after epoch 8330: 0.222381\n",
      "Cost after epoch 8340: 0.222390\n",
      "Cost after epoch 8350: 0.222413\n",
      "Cost after epoch 8360: 0.222403\n",
      "Cost after epoch 8370: 0.222305\n",
      "Cost after epoch 8380: 0.222302\n",
      "Cost after epoch 8390: 0.222297\n",
      "Cost after epoch 8400: 0.222276\n",
      "Cost after epoch 8410: 0.222283\n",
      "Cost after epoch 8420: 0.222312\n",
      "Cost after epoch 8430: 0.222197\n",
      "Cost after epoch 8440: 0.222213\n",
      "Cost after epoch 8450: 0.222252\n",
      "Cost after epoch 8460: 0.222247\n",
      "Cost after epoch 8470: 0.222194\n",
      "Cost after epoch 8480: 0.222163\n",
      "Cost after epoch 8490: 0.222087\n",
      "Cost after epoch 8500: 0.222077\n",
      "Cost after epoch 8510: 0.222094\n",
      "Cost after epoch 8520: 0.222019\n",
      "Cost after epoch 8530: 0.222064\n",
      "Cost after epoch 8540: 0.222062\n",
      "Cost after epoch 8550: 0.221994\n",
      "Cost after epoch 8560: 0.221996\n",
      "Cost after epoch 8570: 0.221985\n",
      "Cost after epoch 8580: 0.221944\n",
      "Cost after epoch 8590: 0.221913\n",
      "Cost after epoch 8600: 0.221963\n",
      "Cost after epoch 8610: 0.221985\n",
      "Cost after epoch 8620: 0.221903\n",
      "Cost after epoch 8630: 0.221888\n",
      "Cost after epoch 8640: 0.221892\n",
      "Cost after epoch 8650: 0.221795\n",
      "Cost after epoch 8660: 0.221778\n",
      "Cost after epoch 8670: 0.221745\n",
      "Cost after epoch 8680: 0.221723\n",
      "Cost after epoch 8690: 0.221737\n",
      "Cost after epoch 8700: 0.221726\n",
      "Cost after epoch 8710: 0.221754\n",
      "Cost after epoch 8720: 0.221669\n",
      "Cost after epoch 8730: 0.221649\n",
      "Cost after epoch 8740: 0.221657\n",
      "Cost after epoch 8750: 0.221623\n",
      "Cost after epoch 8760: 0.221592\n",
      "Cost after epoch 8770: 0.221585\n",
      "Cost after epoch 8780: 0.221600\n",
      "Cost after epoch 8790: 0.221597\n",
      "Cost after epoch 8800: 0.221520\n",
      "Cost after epoch 8810: 0.221531\n",
      "Cost after epoch 8820: 0.221587\n",
      "Cost after epoch 8830: 0.221692\n",
      "Cost after epoch 8840: 0.221522\n",
      "Cost after epoch 8850: 0.221442\n",
      "Cost after epoch 8860: 0.221416\n",
      "Cost after epoch 8870: 0.221447\n",
      "Cost after epoch 8880: 0.221367\n",
      "Cost after epoch 8890: 0.221360\n",
      "Cost after epoch 8900: 0.221382\n",
      "Cost after epoch 8910: 0.221330\n",
      "Cost after epoch 8920: 0.221281\n",
      "Cost after epoch 8930: 0.221312\n",
      "Cost after epoch 8940: 0.221292\n",
      "Cost after epoch 8950: 0.221366\n",
      "Cost after epoch 8960: 0.221308\n",
      "Cost after epoch 8970: 0.221251\n",
      "Cost after epoch 8980: 0.221256\n",
      "Cost after epoch 8990: 0.221218\n",
      "Cost after epoch 9000: 0.221147\n",
      "Cost after epoch 9010: 0.221117\n",
      "Cost after epoch 9020: 0.221109\n",
      "Cost after epoch 9030: 0.221126\n",
      "Cost after epoch 9040: 0.221087\n",
      "Cost after epoch 9050: 0.221185\n",
      "Cost after epoch 9060: 0.221051\n",
      "Cost after epoch 9070: 0.221031\n",
      "Cost after epoch 9080: 0.221075\n",
      "Cost after epoch 9090: 0.221025\n",
      "Cost after epoch 9100: 0.221024\n",
      "Cost after epoch 9110: 0.221053\n",
      "Cost after epoch 9120: 0.220976\n",
      "Cost after epoch 9130: 0.220920\n",
      "Cost after epoch 9140: 0.220895\n",
      "Cost after epoch 9150: 0.220858\n",
      "Cost after epoch 9160: 0.220804\n",
      "Cost after epoch 9170: 0.220907\n",
      "Cost after epoch 9180: 0.220824\n",
      "Cost after epoch 9190: 0.220737\n",
      "Cost after epoch 9200: 0.220754\n",
      "Cost after epoch 9210: 0.220694\n",
      "Cost after epoch 9220: 0.220696\n",
      "Cost after epoch 9230: 0.220635\n",
      "Cost after epoch 9240: 0.220672\n",
      "Cost after epoch 9250: 0.220674\n",
      "Cost after epoch 9260: 0.220555\n",
      "Cost after epoch 9270: 0.220630\n",
      "Cost after epoch 9280: 0.220581\n",
      "Cost after epoch 9290: 0.220521\n",
      "Cost after epoch 9300: 0.220455\n",
      "Cost after epoch 9310: 0.220532\n",
      "Cost after epoch 9320: 0.220424\n",
      "Cost after epoch 9330: 0.220424\n",
      "Cost after epoch 9340: 0.220407\n",
      "Cost after epoch 9350: 0.220355\n",
      "Cost after epoch 9360: 0.220389\n",
      "Cost after epoch 9370: 0.220344\n",
      "Cost after epoch 9380: 0.220435\n",
      "Cost after epoch 9390: 0.220361\n",
      "Cost after epoch 9400: 0.220265\n",
      "Cost after epoch 9410: 0.220230\n",
      "Cost after epoch 9420: 0.220354\n",
      "Cost after epoch 9430: 0.220199\n",
      "Cost after epoch 9440: 0.220128\n",
      "Cost after epoch 9450: 0.220326\n",
      "Cost after epoch 9460: 0.220221\n",
      "Cost after epoch 9470: 0.220164\n",
      "Cost after epoch 9480: 0.220047\n",
      "Cost after epoch 9490: 0.220065\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochsTrain = 9500\n",
    "parameters, costs = model(train_X, train_Y, learning_rate = 0.0009,num_epochs = num_epochsTrain)\n",
    "plt.plot(np.arange(num_epochsTrain)+1, costs)\n",
    "plt.xlabel(\"Iterations(m)\")\n",
    "plt.ylabel(\"Cost function(J)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e63d9",
   "metadata": {},
   "source": [
    "Now, we test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e91926b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.642276</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085714</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.543860</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943089</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.482456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.657143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.828571    1   3  0.593023  0.469298    1        0  0.642276      0   \n",
       "1  0.085714    1   2  0.418605  0.543860    0        1  0.943089      0   \n",
       "2  0.200000    0   1  0.418605  0.342105    0        0  0.821138      0   \n",
       "3  0.628571    1   1  0.302326  0.482456    0        1  0.869919      0   \n",
       "4  0.657143    0   0  0.302326  1.000000    0        1  0.747967      1   \n",
       "\n",
       "   oldpeak  slope  ca  thal  \n",
       "0      2.3      0   0     1  \n",
       "1      3.5      0   0     2  \n",
       "2      1.4      2   0     2  \n",
       "3      0.8      2   0     2  \n",
       "4      0.6      2   0     2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing = pd.read_csv(\"./Dataset/TestingDataset.csv\")\n",
    "for column in columnsToNormalize:\n",
    "    df_testing[column] = MinMaxScaler().fit_transform(np.array(df_testing[column]).reshape(-1,1))\n",
    "\n",
    "test_Y = np.array(df_testing['target'])\n",
    "df_testing.drop(['target'], axis=1, inplace=True)\n",
    "test_Y = test_Y.astype('float32')\n",
    "test_Y = test_Y.reshape(test_Y.shape[0],1)\n",
    "test_X = np.array(df_testing.values)\n",
    "test_X = test_X.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d667f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the testing Dataset was of 89: \n",
      "The accuracy on the train Dataset was of 90: \n"
     ]
    }
   ],
   "source": [
    "Z3 = forward_propagation(tf.transpose(test_X), parameters)\n",
    "A3 = tf.keras.activations.sigmoid(Z3)\n",
    "A3 = tf.math.round(A3)\n",
    "trueVector = A3 == test_Y.transpose()\n",
    "# print(test_Y.sum())\n",
    "# print(test_Y.shape[0])\n",
    "goodPredictionsCounter = trueVector.numpy().sum()\n",
    "perc = (goodPredictionsCounter/test_Y.shape[0]) * 100\n",
    "print(\"The accuracy on the testing Dataset was of %i: \" % (perc))\n",
    "\n",
    "Z3 = forward_propagation(tf.transpose(train_X), parameters)\n",
    "A3 = tf.keras.activations.sigmoid(Z3)\n",
    "A3 = tf.math.round(A3)\n",
    "trueVector = A3 == train_Y.transpose()\n",
    "goodPredictionsCounter = trueVector.numpy().sum()\n",
    "perc = (goodPredictionsCounter/train_Y.shape[0]) * 100\n",
    "print(\"The accuracy on the train Dataset was of %i: \" % (perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c396e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
