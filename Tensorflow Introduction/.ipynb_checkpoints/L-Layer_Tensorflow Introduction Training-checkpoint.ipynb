{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7d0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42b05308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers):\n",
    "    # Define He Initializer for ReLu activations and GlorotNormal for last unit that will be a sigmoid activation\n",
    "    initializerHe = tf.keras.initializers.HeNormal(seed=1)\n",
    "    initializerGn = tf.keras.initializers.GlorotNormal(seed=1)\n",
    "    \n",
    "    parameters = {}\n",
    "    dims = len(layers) - 1\n",
    "    \n",
    "    for i in range(1, dims):\n",
    "        parameters[\"W\" + str(i)] = tf.Variable(initializerHe(shape=(layers[i], layers[i-1])))\n",
    "        parameters[\"b\" + str(i)] = tf.Variable(initializerHe(shape=(layers[i], 1)))\n",
    "\n",
    "    parameters[\"W\" + str(dims)] = tf.Variable(initializerGn(shape=(layers[dims], layers[dims-1])))\n",
    "    parameters[\"b\" + str(dims)] = tf.Variable(initializerGn(shape=(layers[dims], 1)))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70db350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': <tf.Variable 'Variable:0' shape=(10, 13) dtype=float32, numpy=\n",
      "array([[ 0.06439073, -0.2506563 ,  0.32090592, -0.40106028, -0.46258327,\n",
      "        -0.20294088, -0.0193803 ,  0.23794131, -0.00762046, -0.71328866,\n",
      "         0.8716632 , -0.70061535,  0.19026978],\n",
      "       [ 0.34706226, -0.10948427, -0.26533306, -0.08479563, -0.29952168,\n",
      "        -0.00447669, -0.1092835 ,  0.14877783,  0.03333988, -0.09162068,\n",
      "         0.5977041 ,  0.3480738 ,  0.53042334],\n",
      "       [ 0.1255909 ,  0.37428537,  0.01557167, -0.674318  , -0.08367024,\n",
      "         0.01026453,  0.27419996, -0.4339634 , -0.15213068, -0.20063405,\n",
      "        -0.53623706, -0.43535993, -0.29751697],\n",
      "       [-0.5824216 ,  0.82517034,  0.08212346,  0.27582204,  0.7849591 ,\n",
      "         0.36793438, -0.15945162,  0.381502  , -0.36267906,  0.21312976,\n",
      "        -0.1979997 ,  0.14408536,  0.2477363 ],\n",
      "       [-0.728163  , -0.2747505 , -0.32440132, -0.24019793,  0.15501687,\n",
      "        -0.27034608, -0.6305018 ,  0.22762525,  0.42153302,  0.27486998,\n",
      "         0.7935689 , -0.05705784, -0.21752447],\n",
      "       [ 0.54620105,  0.50023246, -0.23278055,  0.282995  , -0.7692628 ,\n",
      "        -0.22206582, -0.22924876, -0.9678465 , -0.31913495, -0.08918583,\n",
      "        -0.23025489,  0.37566498, -0.25829417],\n",
      "       [-0.8887821 ,  0.15038283, -0.71221983, -0.12269241,  0.7103968 ,\n",
      "         0.14291649,  0.9929602 , -0.03219449,  0.6823986 ,  0.17659889,\n",
      "         0.34536064,  0.00705652,  0.08219805],\n",
      "       [ 0.19671935,  0.06692076, -0.44835487, -0.4554599 , -0.14319332,\n",
      "        -0.5907931 , -0.22495103,  0.63297504, -0.18296637,  0.30724603,\n",
      "         0.07577626,  0.92453706,  0.43583122],\n",
      "       [ 0.07128511, -0.99922   , -0.03355718, -0.60039717, -0.40899467,\n",
      "         0.10027833,  0.09436876, -0.5596396 , -0.05899499,  0.92141634,\n",
      "         0.6141824 ,  0.09423622,  0.15660468],\n",
      "       [ 0.6161547 , -0.04849119,  0.1932795 , -0.10257267, -0.6560765 ,\n",
      "        -0.26791978,  0.23818007, -0.79443717, -0.28872162,  0.07627423,\n",
      "        -0.92243606, -0.15340956,  0.1872571 ]], dtype=float32)>, 'b1': <tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
      "array([[ 0.06439073],\n",
      "       [-0.2506563 ],\n",
      "       [ 0.32090592],\n",
      "       [-0.40106028],\n",
      "       [-0.46258327],\n",
      "       [-0.20294088],\n",
      "       [-0.0193803 ],\n",
      "       [ 0.23794131],\n",
      "       [-0.00762046],\n",
      "       [-0.71328866]], dtype=float32)>, 'W2': <tf.Variable 'Variable:0' shape=(1, 10) dtype=float32, numpy=\n",
      "array([[ 0.06139415, -0.2389914 ,  0.30597177, -0.38239595, -0.44105583,\n",
      "        -0.19349653, -0.01847839,  0.22686812, -0.00726582, -0.680094  ]],\n",
      "      dtype=float32)>, 'b2': <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.14398205]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "print(initialize_parameters([13, 10, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b0960d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layers):\n",
    "    dims = len(layers) - 1\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    Z_temp = tf.math.add(tf.linalg.matmul(W1,X), b1)\n",
    "    A_temp = tf.keras.activations.relu(Z_temp)\n",
    "    \n",
    "    for i in range(2, dims):\n",
    "        Z_temp = tf.math.add(tf.linalg.matmul(parameters[\"W\" + str(i)],A_temp), parameters[\"b\" + str(i)])\n",
    "        A_temp = tf.keras.activations.relu(Z_temp)\n",
    "        \n",
    "    Z_temp = tf.math.add(tf.linalg.matmul(parameters[\"W\" + str(dims)],A_temp), parameters[\"b\" + str(dims)])\n",
    "    A_temp = tf.keras.activations.sigmoid(Z_temp)\n",
    "\n",
    "    return Z_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bcd1dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(predicted_Y, true_Y):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(true_Y, predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c77e1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, learning_rate = 0.0001, num_epochs = 1500, layers = [1], print_cost = True):\n",
    "    \n",
    "    # Save all the costs around the training\n",
    "    costs = []\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    parameters = initialize_parameters(layers)\n",
    "    trainable_variables = []\n",
    "    for i in range(1, len(layers)-1):\n",
    "        trainable_variables.append(parameters[\"W\" + str(i)])\n",
    "        trainable_variables.append(parameters[\"b\" + str(i)])\n",
    "    # print(trainable_variables)\n",
    "    \n",
    "    # I want to use the Adam optimizer in my model, in my experience, it's very efficient\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Do the fordward propagation\n",
    "            Z3 = forward_propagation(X_train, parameters, layers)\n",
    "\n",
    "            # Compute the cost function\n",
    "            cost = compute_cost(Z3, Y_train)\n",
    "\n",
    "        grads = tape.gradient(cost, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8858f457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart = pd.read_csv(\"./Dataset/HeartDataset.csv\")\n",
    "df_normalized = df_heart.copy()\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9942d9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333    1   3  0.481132  0.244292    1        0  0.603053      0   \n",
       "1  0.166667    1   2  0.339623  0.283105    0        1  0.885496      0   \n",
       "2  0.250000    0   1  0.339623  0.178082    0        0  0.770992      0   \n",
       "3  0.562500    1   1  0.245283  0.251142    0        1  0.816794      0   \n",
       "4  0.583333    0   0  0.245283  0.520548    0        1  0.702290      1   \n",
       "\n",
       "   oldpeak  slope  ca  thal  target  \n",
       "0      2.3      0   0     1       1  \n",
       "1      3.5      0   0     2       1  \n",
       "2      1.4      2   0     2       1  \n",
       "3      0.8      2   0     2       1  \n",
       "4      0.6      2   0     2       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columnsToNormalize = ['age', 'trestbps', 'chol', 'thalach']\n",
    "for column in columnsToNormalize:\n",
    "    df_normalized[column] = MinMaxScaler().fit_transform(np.array(df_normalized[column]).reshape(-1,1))\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aab637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 299)\n"
     ]
    }
   ],
   "source": [
    "train_Y = np.array(df_heart['target'])\n",
    "df_normalized.drop(['target'], axis=1, inplace=True)\n",
    "train_Y = train_Y.astype('float32')\n",
    "train_Y = train_Y.reshape(299,1)\n",
    "train_Y = tf.transpose(train_Y)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48f5dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 299)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(df_normalized.values)\n",
    "train_X = train_X.astype('float32')\n",
    "train_X = tf.transpose(train_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0e616a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.072925\n",
      "Cost after epoch 10: 1.015951\n",
      "Cost after epoch 20: 0.963340\n",
      "Cost after epoch 30: 0.915678\n",
      "Cost after epoch 40: 0.871998\n",
      "Cost after epoch 50: 0.831152\n",
      "Cost after epoch 60: 0.793443\n",
      "Cost after epoch 70: 0.759120\n",
      "Cost after epoch 80: 0.727905\n",
      "Cost after epoch 90: 0.699363\n",
      "Cost after epoch 100: 0.673022\n",
      "Cost after epoch 110: 0.648549\n",
      "Cost after epoch 120: 0.626141\n",
      "Cost after epoch 130: 0.605917\n",
      "Cost after epoch 140: 0.587569\n",
      "Cost after epoch 150: 0.570822\n",
      "Cost after epoch 160: 0.555610\n",
      "Cost after epoch 170: 0.541525\n",
      "Cost after epoch 180: 0.528677\n",
      "Cost after epoch 190: 0.516948\n",
      "Cost after epoch 200: 0.506334\n",
      "Cost after epoch 210: 0.496610\n",
      "Cost after epoch 220: 0.487706\n",
      "Cost after epoch 230: 0.479530\n",
      "Cost after epoch 240: 0.472005\n",
      "Cost after epoch 250: 0.465238\n",
      "Cost after epoch 260: 0.459006\n",
      "Cost after epoch 270: 0.453217\n",
      "Cost after epoch 280: 0.447845\n",
      "Cost after epoch 290: 0.442995\n",
      "Cost after epoch 300: 0.438553\n",
      "Cost after epoch 310: 0.434446\n",
      "Cost after epoch 320: 0.430608\n",
      "Cost after epoch 330: 0.427032\n",
      "Cost after epoch 340: 0.423652\n",
      "Cost after epoch 350: 0.420483\n",
      "Cost after epoch 360: 0.417480\n",
      "Cost after epoch 370: 0.414608\n",
      "Cost after epoch 380: 0.411865\n",
      "Cost after epoch 390: 0.409201\n",
      "Cost after epoch 400: 0.406671\n",
      "Cost after epoch 410: 0.404263\n",
      "Cost after epoch 420: 0.401959\n",
      "Cost after epoch 430: 0.399772\n",
      "Cost after epoch 440: 0.397686\n",
      "Cost after epoch 450: 0.395669\n",
      "Cost after epoch 460: 0.393729\n",
      "Cost after epoch 470: 0.391861\n",
      "Cost after epoch 480: 0.390063\n",
      "Cost after epoch 490: 0.388343\n",
      "Cost after epoch 500: 0.386684\n",
      "Cost after epoch 510: 0.385094\n",
      "Cost after epoch 520: 0.383573\n",
      "Cost after epoch 530: 0.382116\n",
      "Cost after epoch 540: 0.380723\n",
      "Cost after epoch 550: 0.379388\n",
      "Cost after epoch 560: 0.378117\n",
      "Cost after epoch 570: 0.376909\n",
      "Cost after epoch 580: 0.375756\n",
      "Cost after epoch 590: 0.374659\n",
      "Cost after epoch 600: 0.373614\n",
      "Cost after epoch 610: 0.372612\n",
      "Cost after epoch 620: 0.371641\n",
      "Cost after epoch 630: 0.370704\n",
      "Cost after epoch 640: 0.369801\n",
      "Cost after epoch 650: 0.368938\n",
      "Cost after epoch 660: 0.368050\n",
      "Cost after epoch 670: 0.367166\n",
      "Cost after epoch 680: 0.366296\n",
      "Cost after epoch 690: 0.365451\n",
      "Cost after epoch 700: 0.364626\n",
      "Cost after epoch 710: 0.363802\n",
      "Cost after epoch 720: 0.362999\n",
      "Cost after epoch 730: 0.362224\n",
      "Cost after epoch 740: 0.361463\n",
      "Cost after epoch 750: 0.360723\n",
      "Cost after epoch 760: 0.360005\n",
      "Cost after epoch 770: 0.359310\n",
      "Cost after epoch 780: 0.358680\n",
      "Cost after epoch 790: 0.358078\n",
      "Cost after epoch 800: 0.357499\n",
      "Cost after epoch 810: 0.356937\n",
      "Cost after epoch 820: 0.356390\n",
      "Cost after epoch 830: 0.355857\n",
      "Cost after epoch 840: 0.355331\n",
      "Cost after epoch 850: 0.354814\n",
      "Cost after epoch 860: 0.354307\n",
      "Cost after epoch 870: 0.353823\n",
      "Cost after epoch 880: 0.353359\n",
      "Cost after epoch 890: 0.352907\n",
      "Cost after epoch 900: 0.352459\n",
      "Cost after epoch 910: 0.352025\n",
      "Cost after epoch 920: 0.351607\n",
      "Cost after epoch 930: 0.351203\n",
      "Cost after epoch 940: 0.350808\n",
      "Cost after epoch 950: 0.350426\n",
      "Cost after epoch 960: 0.350059\n",
      "Cost after epoch 970: 0.349704\n",
      "Cost after epoch 980: 0.349360\n",
      "Cost after epoch 990: 0.349026\n",
      "Cost after epoch 1000: 0.348699\n",
      "Cost after epoch 1010: 0.348381\n",
      "Cost after epoch 1020: 0.348072\n",
      "Cost after epoch 1030: 0.347772\n",
      "Cost after epoch 1040: 0.347475\n",
      "Cost after epoch 1050: 0.347174\n",
      "Cost after epoch 1060: 0.346877\n",
      "Cost after epoch 1070: 0.346584\n",
      "Cost after epoch 1080: 0.346302\n",
      "Cost after epoch 1090: 0.346026\n",
      "Cost after epoch 1100: 0.345759\n",
      "Cost after epoch 1110: 0.345501\n",
      "Cost after epoch 1120: 0.345248\n",
      "Cost after epoch 1130: 0.344998\n",
      "Cost after epoch 1140: 0.344759\n",
      "Cost after epoch 1150: 0.344522\n",
      "Cost after epoch 1160: 0.344288\n",
      "Cost after epoch 1170: 0.344059\n",
      "Cost after epoch 1180: 0.343738\n",
      "Cost after epoch 1190: 0.343422\n",
      "Cost after epoch 1200: 0.343076\n",
      "Cost after epoch 1210: 0.342720\n",
      "Cost after epoch 1220: 0.342376\n",
      "Cost after epoch 1230: 0.342055\n",
      "Cost after epoch 1240: 0.341655\n",
      "Cost after epoch 1250: 0.341220\n",
      "Cost after epoch 1260: 0.340802\n",
      "Cost after epoch 1270: 0.340368\n",
      "Cost after epoch 1280: 0.339935\n",
      "Cost after epoch 1290: 0.339506\n",
      "Cost after epoch 1300: 0.339075\n",
      "Cost after epoch 1310: 0.338642\n",
      "Cost after epoch 1320: 0.338203\n",
      "Cost after epoch 1330: 0.337755\n",
      "Cost after epoch 1340: 0.337320\n",
      "Cost after epoch 1350: 0.336901\n",
      "Cost after epoch 1360: 0.336487\n",
      "Cost after epoch 1370: 0.336084\n",
      "Cost after epoch 1380: 0.335707\n",
      "Cost after epoch 1390: 0.335334\n",
      "Cost after epoch 1400: 0.334962\n",
      "Cost after epoch 1410: 0.334587\n",
      "Cost after epoch 1420: 0.334215\n",
      "Cost after epoch 1430: 0.333850\n",
      "Cost after epoch 1440: 0.333482\n",
      "Cost after epoch 1450: 0.333128\n",
      "Cost after epoch 1460: 0.332771\n",
      "Cost after epoch 1470: 0.332440\n",
      "Cost after epoch 1480: 0.332118\n",
      "Cost after epoch 1490: 0.331795\n",
      "Cost after epoch 1500: 0.331480\n",
      "Cost after epoch 1510: 0.331166\n",
      "Cost after epoch 1520: 0.330856\n",
      "Cost after epoch 1530: 0.330547\n",
      "Cost after epoch 1540: 0.330241\n",
      "Cost after epoch 1550: 0.329943\n",
      "Cost after epoch 1560: 0.329643\n",
      "Cost after epoch 1570: 0.329357\n",
      "Cost after epoch 1580: 0.329074\n",
      "Cost after epoch 1590: 0.328792\n",
      "Cost after epoch 1600: 0.328473\n",
      "Cost after epoch 1610: 0.328141\n",
      "Cost after epoch 1620: 0.327808\n",
      "Cost after epoch 1630: 0.327484\n",
      "Cost after epoch 1640: 0.327163\n",
      "Cost after epoch 1650: 0.326845\n",
      "Cost after epoch 1660: 0.326543\n",
      "Cost after epoch 1670: 0.326251\n",
      "Cost after epoch 1680: 0.325945\n",
      "Cost after epoch 1690: 0.325645\n",
      "Cost after epoch 1700: 0.325328\n",
      "Cost after epoch 1710: 0.325009\n",
      "Cost after epoch 1720: 0.324707\n",
      "Cost after epoch 1730: 0.324401\n",
      "Cost after epoch 1740: 0.324103\n",
      "Cost after epoch 1750: 0.323801\n",
      "Cost after epoch 1760: 0.323501\n",
      "Cost after epoch 1770: 0.323212\n",
      "Cost after epoch 1780: 0.322923\n",
      "Cost after epoch 1790: 0.322630\n",
      "Cost after epoch 1800: 0.322300\n",
      "Cost after epoch 1810: 0.321970\n",
      "Cost after epoch 1820: 0.321627\n",
      "Cost after epoch 1830: 0.321295\n",
      "Cost after epoch 1840: 0.320985\n",
      "Cost after epoch 1850: 0.320669\n",
      "Cost after epoch 1860: 0.320379\n",
      "Cost after epoch 1870: 0.320086\n",
      "Cost after epoch 1880: 0.319788\n",
      "Cost after epoch 1890: 0.319505\n",
      "Cost after epoch 1900: 0.319210\n",
      "Cost after epoch 1910: 0.318918\n",
      "Cost after epoch 1920: 0.318629\n",
      "Cost after epoch 1930: 0.318345\n",
      "Cost after epoch 1940: 0.318059\n",
      "Cost after epoch 1950: 0.317783\n",
      "Cost after epoch 1960: 0.317506\n",
      "Cost after epoch 1970: 0.317227\n",
      "Cost after epoch 1980: 0.316945\n",
      "Cost after epoch 1990: 0.316676\n",
      "Cost after epoch 2000: 0.316403\n",
      "Cost after epoch 2010: 0.316133\n",
      "Cost after epoch 2020: 0.315868\n",
      "Cost after epoch 2030: 0.315596\n",
      "Cost after epoch 2040: 0.315325\n",
      "Cost after epoch 2050: 0.315057\n",
      "Cost after epoch 2060: 0.314794\n",
      "Cost after epoch 2070: 0.314524\n",
      "Cost after epoch 2080: 0.314260\n",
      "Cost after epoch 2090: 0.313992\n",
      "Cost after epoch 2100: 0.313724\n",
      "Cost after epoch 2110: 0.313465\n",
      "Cost after epoch 2120: 0.313191\n",
      "Cost after epoch 2130: 0.312916\n",
      "Cost after epoch 2140: 0.312647\n",
      "Cost after epoch 2150: 0.312377\n",
      "Cost after epoch 2160: 0.312105\n",
      "Cost after epoch 2170: 0.311834\n",
      "Cost after epoch 2180: 0.311562\n",
      "Cost after epoch 2190: 0.311295\n",
      "Cost after epoch 2200: 0.311029\n",
      "Cost after epoch 2210: 0.310766\n",
      "Cost after epoch 2220: 0.310502\n",
      "Cost after epoch 2230: 0.310246\n",
      "Cost after epoch 2240: 0.309977\n",
      "Cost after epoch 2250: 0.309714\n",
      "Cost after epoch 2260: 0.309459\n",
      "Cost after epoch 2270: 0.309202\n",
      "Cost after epoch 2280: 0.308939\n",
      "Cost after epoch 2290: 0.308678\n",
      "Cost after epoch 2300: 0.308416\n",
      "Cost after epoch 2310: 0.308150\n",
      "Cost after epoch 2320: 0.307900\n",
      "Cost after epoch 2330: 0.307628\n",
      "Cost after epoch 2340: 0.307360\n",
      "Cost after epoch 2350: 0.307094\n",
      "Cost after epoch 2360: 0.306830\n",
      "Cost after epoch 2370: 0.306570\n",
      "Cost after epoch 2380: 0.306308\n",
      "Cost after epoch 2390: 0.306049\n",
      "Cost after epoch 2400: 0.305786\n",
      "Cost after epoch 2410: 0.305521\n",
      "Cost after epoch 2420: 0.305258\n",
      "Cost after epoch 2430: 0.304975\n",
      "Cost after epoch 2440: 0.304697\n",
      "Cost after epoch 2450: 0.304408\n",
      "Cost after epoch 2460: 0.304127\n",
      "Cost after epoch 2470: 0.303846\n",
      "Cost after epoch 2480: 0.303567\n",
      "Cost after epoch 2490: 0.303281\n",
      "Cost after epoch 2500: 0.303007\n",
      "Cost after epoch 2510: 0.302733\n",
      "Cost after epoch 2520: 0.302453\n",
      "Cost after epoch 2530: 0.302187\n",
      "Cost after epoch 2540: 0.301902\n",
      "Cost after epoch 2550: 0.301623\n",
      "Cost after epoch 2560: 0.301338\n",
      "Cost after epoch 2570: 0.301060\n",
      "Cost after epoch 2580: 0.300788\n",
      "Cost after epoch 2590: 0.300511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2600: 0.300224\n",
      "Cost after epoch 2610: 0.299940\n",
      "Cost after epoch 2620: 0.299669\n",
      "Cost after epoch 2630: 0.299382\n",
      "Cost after epoch 2640: 0.299105\n",
      "Cost after epoch 2650: 0.298821\n",
      "Cost after epoch 2660: 0.298540\n",
      "Cost after epoch 2670: 0.298268\n",
      "Cost after epoch 2680: 0.297984\n",
      "Cost after epoch 2690: 0.297708\n",
      "Cost after epoch 2700: 0.297429\n",
      "Cost after epoch 2710: 0.297149\n",
      "Cost after epoch 2720: 0.296873\n",
      "Cost after epoch 2730: 0.296592\n",
      "Cost after epoch 2740: 0.296315\n",
      "Cost after epoch 2750: 0.296041\n",
      "Cost after epoch 2760: 0.295762\n",
      "Cost after epoch 2770: 0.295484\n",
      "Cost after epoch 2780: 0.295210\n",
      "Cost after epoch 2790: 0.294937\n",
      "Cost after epoch 2800: 0.294658\n",
      "Cost after epoch 2810: 0.294383\n",
      "Cost after epoch 2820: 0.294116\n",
      "Cost after epoch 2830: 0.293843\n",
      "Cost after epoch 2840: 0.293563\n",
      "Cost after epoch 2850: 0.293290\n",
      "Cost after epoch 2860: 0.293027\n",
      "Cost after epoch 2870: 0.292744\n",
      "Cost after epoch 2880: 0.292465\n",
      "Cost after epoch 2890: 0.292193\n",
      "Cost after epoch 2900: 0.291924\n",
      "Cost after epoch 2910: 0.291649\n",
      "Cost after epoch 2920: 0.291374\n",
      "Cost after epoch 2930: 0.291100\n",
      "Cost after epoch 2940: 0.290825\n",
      "Cost after epoch 2950: 0.290546\n",
      "Cost after epoch 2960: 0.290261\n",
      "Cost after epoch 2970: 0.289964\n",
      "Cost after epoch 2980: 0.289648\n",
      "Cost after epoch 2990: 0.289319\n",
      "Cost after epoch 3000: 0.288986\n",
      "Cost after epoch 3010: 0.288650\n",
      "Cost after epoch 3020: 0.288284\n",
      "Cost after epoch 3030: 0.287951\n",
      "Cost after epoch 3040: 0.287616\n",
      "Cost after epoch 3050: 0.287301\n",
      "Cost after epoch 3060: 0.286974\n",
      "Cost after epoch 3070: 0.286668\n",
      "Cost after epoch 3080: 0.286351\n",
      "Cost after epoch 3090: 0.286044\n",
      "Cost after epoch 3100: 0.285734\n",
      "Cost after epoch 3110: 0.285423\n",
      "Cost after epoch 3120: 0.285111\n",
      "Cost after epoch 3130: 0.284803\n",
      "Cost after epoch 3140: 0.284507\n",
      "Cost after epoch 3150: 0.284204\n",
      "Cost after epoch 3160: 0.283905\n",
      "Cost after epoch 3170: 0.283601\n",
      "Cost after epoch 3180: 0.283307\n",
      "Cost after epoch 3190: 0.283015\n",
      "Cost after epoch 3200: 0.282724\n",
      "Cost after epoch 3210: 0.282423\n",
      "Cost after epoch 3220: 0.282142\n",
      "Cost after epoch 3230: 0.281863\n",
      "Cost after epoch 3240: 0.281594\n",
      "Cost after epoch 3250: 0.281342\n",
      "Cost after epoch 3260: 0.281098\n",
      "Cost after epoch 3270: 0.280830\n",
      "Cost after epoch 3280: 0.280580\n",
      "Cost after epoch 3290: 0.280330\n",
      "Cost after epoch 3300: 0.280090\n",
      "Cost after epoch 3310: 0.279837\n",
      "Cost after epoch 3320: 0.279589\n",
      "Cost after epoch 3330: 0.279354\n",
      "Cost after epoch 3340: 0.279111\n",
      "Cost after epoch 3350: 0.278870\n",
      "Cost after epoch 3360: 0.278620\n",
      "Cost after epoch 3370: 0.278378\n",
      "Cost after epoch 3380: 0.278148\n",
      "Cost after epoch 3390: 0.277921\n",
      "Cost after epoch 3400: 0.277686\n",
      "Cost after epoch 3410: 0.277451\n",
      "Cost after epoch 3420: 0.277217\n",
      "Cost after epoch 3430: 0.276992\n",
      "Cost after epoch 3440: 0.276763\n",
      "Cost after epoch 3450: 0.276531\n",
      "Cost after epoch 3460: 0.276311\n",
      "Cost after epoch 3470: 0.276084\n",
      "Cost after epoch 3480: 0.275864\n",
      "Cost after epoch 3490: 0.275645\n",
      "Cost after epoch 3500: 0.275423\n",
      "Cost after epoch 3510: 0.275207\n",
      "Cost after epoch 3520: 0.274987\n",
      "Cost after epoch 3530: 0.274762\n",
      "Cost after epoch 3540: 0.274545\n",
      "Cost after epoch 3550: 0.274336\n",
      "Cost after epoch 3560: 0.274119\n",
      "Cost after epoch 3570: 0.273906\n",
      "Cost after epoch 3580: 0.273685\n",
      "Cost after epoch 3590: 0.273450\n",
      "Cost after epoch 3600: 0.273221\n",
      "Cost after epoch 3610: 0.272986\n",
      "Cost after epoch 3620: 0.272753\n",
      "Cost after epoch 3630: 0.272516\n",
      "Cost after epoch 3640: 0.272288\n",
      "Cost after epoch 3650: 0.272047\n",
      "Cost after epoch 3660: 0.271817\n",
      "Cost after epoch 3670: 0.271584\n",
      "Cost after epoch 3680: 0.271335\n",
      "Cost after epoch 3690: 0.271121\n",
      "Cost after epoch 3700: 0.270871\n",
      "Cost after epoch 3710: 0.270571\n",
      "Cost after epoch 3720: 0.270269\n",
      "Cost after epoch 3730: 0.269969\n",
      "Cost after epoch 3740: 0.269662\n",
      "Cost after epoch 3750: 0.269373\n",
      "Cost after epoch 3760: 0.269087\n",
      "Cost after epoch 3770: 0.268799\n",
      "Cost after epoch 3780: 0.268520\n",
      "Cost after epoch 3790: 0.268248\n",
      "Cost after epoch 3800: 0.267985\n",
      "Cost after epoch 3810: 0.267721\n",
      "Cost after epoch 3820: 0.267432\n",
      "Cost after epoch 3830: 0.267125\n",
      "Cost after epoch 3840: 0.266855\n",
      "Cost after epoch 3850: 0.266572\n",
      "Cost after epoch 3860: 0.266313\n",
      "Cost after epoch 3870: 0.266046\n",
      "Cost after epoch 3880: 0.265780\n",
      "Cost after epoch 3890: 0.265523\n",
      "Cost after epoch 3900: 0.265260\n",
      "Cost after epoch 3910: 0.265002\n",
      "Cost after epoch 3920: 0.264744\n",
      "Cost after epoch 3930: 0.264492\n",
      "Cost after epoch 3940: 0.264237\n",
      "Cost after epoch 3950: 0.263941\n",
      "Cost after epoch 3960: 0.263663\n",
      "Cost after epoch 3970: 0.263363\n",
      "Cost after epoch 3980: 0.263094\n",
      "Cost after epoch 3990: 0.262813\n",
      "Cost after epoch 4000: 0.262551\n",
      "Cost after epoch 4010: 0.262293\n",
      "Cost after epoch 4020: 0.262014\n",
      "Cost after epoch 4030: 0.261761\n",
      "Cost after epoch 4040: 0.261494\n",
      "Cost after epoch 4050: 0.261240\n",
      "Cost after epoch 4060: 0.260990\n",
      "Cost after epoch 4070: 0.260739\n",
      "Cost after epoch 4080: 0.260490\n",
      "Cost after epoch 4090: 0.260231\n",
      "Cost after epoch 4100: 0.259997\n",
      "Cost after epoch 4110: 0.259744\n",
      "Cost after epoch 4120: 0.259491\n",
      "Cost after epoch 4130: 0.259221\n",
      "Cost after epoch 4140: 0.258952\n",
      "Cost after epoch 4150: 0.258689\n",
      "Cost after epoch 4160: 0.258429\n",
      "Cost after epoch 4170: 0.258163\n",
      "Cost after epoch 4180: 0.257907\n",
      "Cost after epoch 4190: 0.257659\n",
      "Cost after epoch 4200: 0.257402\n",
      "Cost after epoch 4210: 0.257138\n",
      "Cost after epoch 4220: 0.256891\n",
      "Cost after epoch 4230: 0.256646\n",
      "Cost after epoch 4240: 0.256384\n",
      "Cost after epoch 4250: 0.256121\n",
      "Cost after epoch 4260: 0.255833\n",
      "Cost after epoch 4270: 0.255560\n",
      "Cost after epoch 4280: 0.255287\n",
      "Cost after epoch 4290: 0.255039\n",
      "Cost after epoch 4300: 0.254779\n",
      "Cost after epoch 4310: 0.254522\n",
      "Cost after epoch 4320: 0.254257\n",
      "Cost after epoch 4330: 0.254006\n",
      "Cost after epoch 4340: 0.253759\n",
      "Cost after epoch 4350: 0.253525\n",
      "Cost after epoch 4360: 0.253281\n",
      "Cost after epoch 4370: 0.253020\n",
      "Cost after epoch 4380: 0.252726\n",
      "Cost after epoch 4390: 0.252452\n",
      "Cost after epoch 4400: 0.252158\n",
      "Cost after epoch 4410: 0.251871\n",
      "Cost after epoch 4420: 0.251615\n",
      "Cost after epoch 4430: 0.251331\n",
      "Cost after epoch 4440: 0.251063\n",
      "Cost after epoch 4450: 0.250802\n",
      "Cost after epoch 4460: 0.250549\n",
      "Cost after epoch 4470: 0.250293\n",
      "Cost after epoch 4480: 0.250045\n",
      "Cost after epoch 4490: 0.249789\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochsTrain = 4500 #9500 #4500 #7200\n",
    "layers_train = [13, 10, 5, 1]\n",
    "parameters, costs = model(train_X, train_Y, learning_rate = 0.0009, layers=layers_train , num_epochs = num_epochsTrain)\n",
    "plt.plot(np.arange(num_epochsTrain)+1, costs)\n",
    "plt.xlabel(\"Iterations(m)\")\n",
    "plt.ylabel(\"Cost function(J)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8af807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 91)\n",
      "(1, 91)\n"
     ]
    }
   ],
   "source": [
    "df_testing = pd.read_csv(\"./Dataset/TestingDataset.csv\")\n",
    "for column in columnsToNormalize:\n",
    "    df_testing[column] = MinMaxScaler().fit_transform(np.array(df_testing[column]).reshape(-1,1))\n",
    "\n",
    "test_Y = np.array(df_testing['target'])\n",
    "df_testing.drop(['target'], axis=1, inplace=True)\n",
    "test_Y = test_Y.astype('float32')\n",
    "test_Y = test_Y.reshape(test_Y.shape[0],1)\n",
    "test_X = np.array(df_testing.values)\n",
    "test_X = test_X.astype('float32')\n",
    "test_X = tf.transpose(test_X)\n",
    "test_Y = tf.transpose(test_Y)\n",
    "\n",
    "df_testing.head()\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3315ca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]], shape=(1, 91), dtype=float32)\n",
      "The accuracy on the testing Dataset was of 90: \n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]], shape=(1, 299), dtype=float32)\n",
      "The accuracy on the train Dataset was of 90: \n"
     ]
    }
   ],
   "source": [
    "Z3 = forward_propagation(test_X, parameters, layers=layers_train)\n",
    "A3 = tf.keras.activations.sigmoid(Z3)\n",
    "A3 = tf.math.round(A3)\n",
    "print(A3)\n",
    "trueVector = A3 == test_Y\n",
    "goodPredictionsCounter = trueVector.numpy().sum()\n",
    "perc = (goodPredictionsCounter/test_Y.shape[1]) * 100\n",
    "print(\"The accuracy on the testing Dataset was of %i: \" % (perc))\n",
    "\n",
    "Z3 = forward_propagation(train_X, parameters, layers=layers_train)\n",
    "A3 = tf.keras.activations.sigmoid(Z3)\n",
    "A3 = tf.math.round(A3)\n",
    "print(A3)\n",
    "trueVector = A3 == train_Y\n",
    "goodPredictionsCounter = trueVector.numpy().sum()\n",
    "perc = (goodPredictionsCounter/train_Y.shape[1]) * 100\n",
    "print(\"The accuracy on the train Dataset was of %i: \" % (perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57d0c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model at the momment\n",
    "best_model = parameters\n",
    "best_layers = layers_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf4bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
