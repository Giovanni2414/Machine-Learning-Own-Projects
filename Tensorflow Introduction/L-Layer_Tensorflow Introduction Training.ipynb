{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c714b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8d130a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers):\n",
    "    # Define He Initializer for ReLu activations and GlorotNormal for last unit that will be a sigmoid activation\n",
    "    initializerHe = tf.keras.initializers.HeNormal(seed=1)\n",
    "    initializerGn = tf.keras.initializers.GlorotNormal(seed=1)\n",
    "    \n",
    "    parameters = {}\n",
    "    dims = len(layers) - 1\n",
    "    \n",
    "    for i in range(1, dims):\n",
    "        parameters[\"W\" + str(i)] = tf.Variable(initializerHe(shape=(layers[i], layers[i-1])))\n",
    "        parameters[\"b\" + str(i)] = tf.Variable(initializerHe(shape=(layers[i], 1)))\n",
    "\n",
    "    parameters[\"W\" + str(dims)] = tf.Variable(initializerGn(shape=(layers[dims], layers[dims-1])))\n",
    "    parameters[\"b\" + str(dims)] = tf.Variable(initializerGn(shape=(layers[dims], 1)))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35387959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': <tf.Variable 'Variable:0' shape=(10, 13) dtype=float32, numpy=\n",
      "array([[ 0.06439073, -0.2506563 ,  0.32090592, -0.40106028, -0.46258327,\n",
      "        -0.20294088, -0.0193803 ,  0.23794131, -0.00762046, -0.71328866,\n",
      "         0.8716632 , -0.70061535,  0.19026978],\n",
      "       [ 0.34706226, -0.10948427, -0.26533306, -0.08479563, -0.29952168,\n",
      "        -0.00447669, -0.1092835 ,  0.14877783,  0.03333988, -0.09162068,\n",
      "         0.5977041 ,  0.3480738 ,  0.53042334],\n",
      "       [ 0.1255909 ,  0.37428537,  0.01557167, -0.674318  , -0.08367024,\n",
      "         0.01026453,  0.27419996, -0.4339634 , -0.15213068, -0.20063405,\n",
      "        -0.53623706, -0.43535993, -0.29751697],\n",
      "       [-0.5824216 ,  0.82517034,  0.08212346,  0.27582204,  0.7849591 ,\n",
      "         0.36793438, -0.15945162,  0.381502  , -0.36267906,  0.21312976,\n",
      "        -0.1979997 ,  0.14408536,  0.2477363 ],\n",
      "       [-0.728163  , -0.2747505 , -0.32440132, -0.24019793,  0.15501687,\n",
      "        -0.27034608, -0.6305018 ,  0.22762525,  0.42153302,  0.27486998,\n",
      "         0.7935689 , -0.05705784, -0.21752447],\n",
      "       [ 0.54620105,  0.50023246, -0.23278055,  0.282995  , -0.7692628 ,\n",
      "        -0.22206582, -0.22924876, -0.9678465 , -0.31913495, -0.08918583,\n",
      "        -0.23025489,  0.37566498, -0.25829417],\n",
      "       [-0.8887821 ,  0.15038283, -0.71221983, -0.12269241,  0.7103968 ,\n",
      "         0.14291649,  0.9929602 , -0.03219449,  0.6823986 ,  0.17659889,\n",
      "         0.34536064,  0.00705652,  0.08219805],\n",
      "       [ 0.19671935,  0.06692076, -0.44835487, -0.4554599 , -0.14319332,\n",
      "        -0.5907931 , -0.22495103,  0.63297504, -0.18296637,  0.30724603,\n",
      "         0.07577626,  0.92453706,  0.43583122],\n",
      "       [ 0.07128511, -0.99922   , -0.03355718, -0.60039717, -0.40899467,\n",
      "         0.10027833,  0.09436876, -0.5596396 , -0.05899499,  0.92141634,\n",
      "         0.6141824 ,  0.09423622,  0.15660468],\n",
      "       [ 0.6161547 , -0.04849119,  0.1932795 , -0.10257267, -0.6560765 ,\n",
      "        -0.26791978,  0.23818007, -0.79443717, -0.28872162,  0.07627423,\n",
      "        -0.92243606, -0.15340956,  0.1872571 ]], dtype=float32)>, 'b1': <tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
      "array([[ 0.06439073],\n",
      "       [-0.2506563 ],\n",
      "       [ 0.32090592],\n",
      "       [-0.40106028],\n",
      "       [-0.46258327],\n",
      "       [-0.20294088],\n",
      "       [-0.0193803 ],\n",
      "       [ 0.23794131],\n",
      "       [-0.00762046],\n",
      "       [-0.71328866]], dtype=float32)>, 'W2': <tf.Variable 'Variable:0' shape=(1, 10) dtype=float32, numpy=\n",
      "array([[ 0.06139415, -0.2389914 ,  0.30597177, -0.38239595, -0.44105583,\n",
      "        -0.19349653, -0.01847839,  0.22686812, -0.00726582, -0.680094  ]],\n",
      "      dtype=float32)>, 'b2': <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.14398205]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "print(initialize_parameters([13, 10, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2b97eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layers):\n",
    "    dims = len(layers) - 1\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    Z_temp = tf.math.add(tf.linalg.matmul(W1,X), b1)\n",
    "    A_temp = tf.keras.activations.relu(Z_temp)\n",
    "    \n",
    "    for i in range(2, dims):\n",
    "        Z_temp = tf.math.add(tf.linalg.matmul(parameters[\"W\" + str(i)],A_temp), parameters[\"b\" + str(i)])\n",
    "        A_temp = tf.keras.activations.relu(Z_temp)\n",
    "        \n",
    "    Z_temp = tf.math.add(tf.linalg.matmul(parameters[\"W\" + str(dims)],A_temp), parameters[\"b\" + str(dims)])\n",
    "    A_temp = tf.keras.activations.sigmoid(Z_temp)\n",
    "\n",
    "    return Z_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17eddfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(predicted_Y, true_Y):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(true_Y, predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07a91654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, learning_rate = 0.0001, num_epochs = 1500, layers = [1], print_cost = True):\n",
    "    \n",
    "    # Save all the costs around the training\n",
    "    costs = []\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    parameters = initialize_parameters(layers)\n",
    "    trainable_variables = []\n",
    "    for i in range(1, len(layers)-1):\n",
    "        trainable_variables.append(parameters[\"W\" + str(i)])\n",
    "        trainable_variables.append(parameters[\"b\" + str(i)])\n",
    "    # print(trainable_variables)\n",
    "    \n",
    "    # I want to use the Adam optimizer in my model, in my experience, it's very efficient\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Do the fordward propagation\n",
    "            Z3 = forward_propagation(X_train, parameters, layers)\n",
    "\n",
    "            # Compute the cost function\n",
    "            cost = compute_cost(Z3, Y_train)\n",
    "\n",
    "        grads = tape.gradient(cost, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78cebb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart = pd.read_csv(\"./Dataset/HeartDataset.csv\")\n",
    "df_normalized = df_heart.copy()\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ebd1bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333    1   3  0.481132  0.244292    1        0  0.603053      0   \n",
       "1  0.166667    1   2  0.339623  0.283105    0        1  0.885496      0   \n",
       "2  0.250000    0   1  0.339623  0.178082    0        0  0.770992      0   \n",
       "3  0.562500    1   1  0.245283  0.251142    0        1  0.816794      0   \n",
       "4  0.583333    0   0  0.245283  0.520548    0        1  0.702290      1   \n",
       "\n",
       "   oldpeak  slope  ca  thal  target  \n",
       "0      2.3      0   0     1       1  \n",
       "1      3.5      0   0     2       1  \n",
       "2      1.4      2   0     2       1  \n",
       "3      0.8      2   0     2       1  \n",
       "4      0.6      2   0     2       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columnsToNormalize = ['age', 'trestbps', 'chol', 'thalach']\n",
    "for column in columnsToNormalize:\n",
    "    df_normalized[column] = MinMaxScaler().fit_transform(np.array(df_normalized[column]).reshape(-1,1))\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a34ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 299)\n"
     ]
    }
   ],
   "source": [
    "train_Y = np.array(df_heart['target'])\n",
    "df_normalized.drop(['target'], axis=1, inplace=True)\n",
    "train_Y = train_Y.astype('float32')\n",
    "train_Y = train_Y.reshape(299,1)\n",
    "train_Y = tf.transpose(train_Y)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541e78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 299)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(df_normalized.values)\n",
    "train_X = train_X.astype('float32')\n",
    "train_X = tf.transpose(train_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b32086e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.676168\n",
      "Cost after epoch 10: 0.654913\n",
      "Cost after epoch 20: 0.636716\n",
      "Cost after epoch 30: 0.621451\n",
      "Cost after epoch 40: 0.606149\n",
      "Cost after epoch 50: 0.591007\n",
      "Cost after epoch 60: 0.576319\n",
      "Cost after epoch 70: 0.561610\n",
      "Cost after epoch 80: 0.547027\n",
      "Cost after epoch 90: 0.532558\n",
      "Cost after epoch 100: 0.517688\n",
      "Cost after epoch 110: 0.503574\n",
      "Cost after epoch 120: 0.491102\n",
      "Cost after epoch 130: 0.479174\n",
      "Cost after epoch 140: 0.467821\n",
      "Cost after epoch 150: 0.456482\n",
      "Cost after epoch 160: 0.444751\n",
      "Cost after epoch 170: 0.431113\n",
      "Cost after epoch 180: 0.416452\n",
      "Cost after epoch 190: 0.402051\n",
      "Cost after epoch 200: 0.388350\n",
      "Cost after epoch 210: 0.375815\n",
      "Cost after epoch 220: 0.364899\n",
      "Cost after epoch 230: 0.355585\n",
      "Cost after epoch 240: 0.347700\n",
      "Cost after epoch 250: 0.340320\n",
      "Cost after epoch 260: 0.333766\n",
      "Cost after epoch 270: 0.327828\n",
      "Cost after epoch 280: 0.322282\n",
      "Cost after epoch 290: 0.317051\n",
      "Cost after epoch 300: 0.312409\n",
      "Cost after epoch 310: 0.308160\n",
      "Cost after epoch 320: 0.304137\n",
      "Cost after epoch 330: 0.300217\n",
      "Cost after epoch 340: 0.296262\n",
      "Cost after epoch 350: 0.292137\n",
      "Cost after epoch 360: 0.288133\n",
      "Cost after epoch 370: 0.284127\n",
      "Cost after epoch 380: 0.280211\n",
      "Cost after epoch 390: 0.276446\n",
      "Cost after epoch 400: 0.272862\n",
      "Cost after epoch 410: 0.269391\n",
      "Cost after epoch 420: 0.266197\n",
      "Cost after epoch 430: 0.263185\n",
      "Cost after epoch 440: 0.260286\n",
      "Cost after epoch 450: 0.257491\n",
      "Cost after epoch 460: 0.254827\n",
      "Cost after epoch 470: 0.252313\n",
      "Cost after epoch 480: 0.249886\n",
      "Cost after epoch 490: 0.247503\n",
      "Cost after epoch 500: 0.245045\n",
      "Cost after epoch 510: 0.242328\n",
      "Cost after epoch 520: 0.239806\n",
      "Cost after epoch 530: 0.237305\n",
      "Cost after epoch 540: 0.234818\n",
      "Cost after epoch 550: 0.231949\n",
      "Cost after epoch 560: 0.228251\n",
      "Cost after epoch 570: 0.224766\n",
      "Cost after epoch 580: 0.222128\n",
      "Cost after epoch 590: 0.219494\n",
      "Cost after epoch 600: 0.217041\n",
      "Cost after epoch 610: 0.214677\n",
      "Cost after epoch 620: 0.212031\n",
      "Cost after epoch 630: 0.209246\n",
      "Cost after epoch 640: 0.206539\n",
      "Cost after epoch 650: 0.203893\n",
      "Cost after epoch 660: 0.201253\n",
      "Cost after epoch 670: 0.198614\n",
      "Cost after epoch 680: 0.196034\n",
      "Cost after epoch 690: 0.193477\n",
      "Cost after epoch 700: 0.190861\n",
      "Cost after epoch 710: 0.188255\n",
      "Cost after epoch 720: 0.185569\n",
      "Cost after epoch 730: 0.182966\n",
      "Cost after epoch 740: 0.180367\n",
      "Cost after epoch 750: 0.177784\n",
      "Cost after epoch 760: 0.175080\n",
      "Cost after epoch 770: 0.172449\n",
      "Cost after epoch 780: 0.169873\n",
      "Cost after epoch 790: 0.167361\n",
      "Cost after epoch 800: 0.164829\n",
      "Cost after epoch 810: 0.162397\n",
      "Cost after epoch 820: 0.159893\n",
      "Cost after epoch 830: 0.157084\n",
      "Cost after epoch 840: 0.153995\n",
      "Cost after epoch 850: 0.151017\n",
      "Cost after epoch 860: 0.147805\n",
      "Cost after epoch 870: 0.144320\n",
      "Cost after epoch 880: 0.140950\n",
      "Cost after epoch 890: 0.137887\n",
      "Cost after epoch 900: 0.134933\n",
      "Cost after epoch 910: 0.132068\n",
      "Cost after epoch 920: 0.129399\n",
      "Cost after epoch 930: 0.126876\n",
      "Cost after epoch 940: 0.124309\n",
      "Cost after epoch 950: 0.121579\n",
      "Cost after epoch 960: 0.118649\n",
      "Cost after epoch 970: 0.116039\n",
      "Cost after epoch 980: 0.113660\n",
      "Cost after epoch 990: 0.111346\n",
      "Cost after epoch 1000: 0.109234\n",
      "Cost after epoch 1010: 0.107137\n",
      "Cost after epoch 1020: 0.105066\n",
      "Cost after epoch 1030: 0.102106\n",
      "Cost after epoch 1040: 0.094632\n",
      "Cost after epoch 1050: 0.091628\n",
      "Cost after epoch 1060: 0.088955\n",
      "Cost after epoch 1070: 0.086605\n",
      "Cost after epoch 1080: 0.084334\n",
      "Cost after epoch 1090: 0.082127\n",
      "Cost after epoch 1100: 0.080054\n",
      "Cost after epoch 1110: 0.078090\n",
      "Cost after epoch 1120: 0.076383\n",
      "Cost after epoch 1130: 0.074788\n",
      "Cost after epoch 1140: 0.073146\n",
      "Cost after epoch 1150: 0.071527\n",
      "Cost after epoch 1160: 0.069764\n",
      "Cost after epoch 1170: 0.068065\n",
      "Cost after epoch 1180: 0.066620\n",
      "Cost after epoch 1190: 0.065234\n",
      "Cost after epoch 1200: 0.064086\n",
      "Cost after epoch 1210: 0.062878\n",
      "Cost after epoch 1220: 0.061842\n",
      "Cost after epoch 1230: 0.060868\n",
      "Cost after epoch 1240: 0.059795\n",
      "Cost after epoch 1250: 0.058905\n",
      "Cost after epoch 1260: 0.057964\n",
      "Cost after epoch 1270: 0.057087\n",
      "Cost after epoch 1280: 0.056263\n",
      "Cost after epoch 1290: 0.055496\n",
      "Cost after epoch 1300: 0.054805\n",
      "Cost after epoch 1310: 0.054066\n",
      "Cost after epoch 1320: 0.053333\n",
      "Cost after epoch 1330: 0.052684\n",
      "Cost after epoch 1340: 0.052018\n",
      "Cost after epoch 1350: 0.051381\n",
      "Cost after epoch 1360: 0.050832\n",
      "Cost after epoch 1370: 0.050227\n",
      "Cost after epoch 1380: 0.049678\n",
      "Cost after epoch 1390: 0.049110\n",
      "Cost after epoch 1400: 0.048575\n",
      "Cost after epoch 1410: 0.048049\n",
      "Cost after epoch 1420: 0.047554\n",
      "Cost after epoch 1430: 0.047077\n",
      "Cost after epoch 1440: 0.046553\n",
      "Cost after epoch 1450: 0.046080\n",
      "Cost after epoch 1460: 0.045592\n",
      "Cost after epoch 1470: 0.045172\n",
      "Cost after epoch 1480: 0.044665\n",
      "Cost after epoch 1490: 0.044227\n",
      "Cost after epoch 1500: 0.043787\n",
      "Cost after epoch 1510: 0.043342\n",
      "Cost after epoch 1520: 0.042902\n",
      "Cost after epoch 1530: 0.042454\n",
      "Cost after epoch 1540: 0.042062\n",
      "Cost after epoch 1550: 0.041686\n",
      "Cost after epoch 1560: 0.041241\n",
      "Cost after epoch 1570: 0.040845\n",
      "Cost after epoch 1580: 0.040456\n",
      "Cost after epoch 1590: 0.040104\n",
      "Cost after epoch 1600: 0.039705\n",
      "Cost after epoch 1610: 0.039330\n",
      "Cost after epoch 1620: 0.038931\n",
      "Cost after epoch 1630: 0.038554\n",
      "Cost after epoch 1640: 0.038214\n",
      "Cost after epoch 1650: 0.037831\n",
      "Cost after epoch 1660: 0.037463\n",
      "Cost after epoch 1670: 0.037106\n",
      "Cost after epoch 1680: 0.036741\n",
      "Cost after epoch 1690: 0.036367\n",
      "Cost after epoch 1700: 0.035999\n",
      "Cost after epoch 1710: 0.035618\n",
      "Cost after epoch 1720: 0.035251\n",
      "Cost after epoch 1730: 0.034883\n",
      "Cost after epoch 1740: 0.034512\n",
      "Cost after epoch 1750: 0.034144\n",
      "Cost after epoch 1760: 0.033779\n",
      "Cost after epoch 1770: 0.033436\n",
      "Cost after epoch 1780: 0.033058\n",
      "Cost after epoch 1790: 0.032692\n",
      "Cost after epoch 1800: 0.032315\n",
      "Cost after epoch 1810: 0.031962\n",
      "Cost after epoch 1820: 0.031602\n",
      "Cost after epoch 1830: 0.031252\n",
      "Cost after epoch 1840: 0.030900\n",
      "Cost after epoch 1850: 0.030577\n",
      "Cost after epoch 1860: 0.030244\n",
      "Cost after epoch 1870: 0.029944\n",
      "Cost after epoch 1880: 0.029615\n",
      "Cost after epoch 1890: 0.029300\n",
      "Cost after epoch 1900: 0.029011\n",
      "Cost after epoch 1910: 0.028747\n",
      "Cost after epoch 1920: 0.028434\n",
      "Cost after epoch 1930: 0.028153\n",
      "Cost after epoch 1940: 0.027860\n",
      "Cost after epoch 1950: 0.027599\n",
      "Cost after epoch 1960: 0.027349\n",
      "Cost after epoch 1970: 0.027110\n",
      "Cost after epoch 1980: 0.026833\n",
      "Cost after epoch 1990: 0.026569\n",
      "Cost after epoch 2000: 0.026367\n",
      "Cost after epoch 2010: 0.026120\n",
      "Cost after epoch 2020: 0.025875\n",
      "Cost after epoch 2030: 0.025643\n",
      "Cost after epoch 2040: 0.025461\n",
      "Cost after epoch 2050: 0.025217\n",
      "Cost after epoch 2060: 0.024999\n",
      "Cost after epoch 2070: 0.024838\n",
      "Cost after epoch 2080: 0.024634\n",
      "Cost after epoch 2090: 0.024443\n",
      "Cost after epoch 2100: 0.024257\n",
      "Cost after epoch 2110: 0.024090\n",
      "Cost after epoch 2120: 0.023899\n",
      "Cost after epoch 2130: 0.023755\n",
      "Cost after epoch 2140: 0.023605\n",
      "Cost after epoch 2150: 0.023437\n",
      "Cost after epoch 2160: 0.023256\n",
      "Cost after epoch 2170: 0.023137\n",
      "Cost after epoch 2180: 0.022969\n",
      "Cost after epoch 2190: 0.022818\n",
      "Cost after epoch 2200: 0.022691\n",
      "Cost after epoch 2210: 0.022542\n",
      "Cost after epoch 2220: 0.022414\n",
      "Cost after epoch 2230: 0.022260\n",
      "Cost after epoch 2240: 0.022147\n",
      "Cost after epoch 2250: 0.022013\n",
      "Cost after epoch 2260: 0.021896\n",
      "Cost after epoch 2270: 0.021759\n",
      "Cost after epoch 2280: 0.021636\n",
      "Cost after epoch 2290: 0.021530\n",
      "Cost after epoch 2300: 0.021417\n",
      "Cost after epoch 2310: 0.021331\n",
      "Cost after epoch 2320: 0.021230\n",
      "Cost after epoch 2330: 0.021146\n",
      "Cost after epoch 2340: 0.021079\n",
      "Cost after epoch 2350: 0.020961\n",
      "Cost after epoch 2360: 0.020865\n",
      "Cost after epoch 2370: 0.020778\n",
      "Cost after epoch 2380: 0.020706\n",
      "Cost after epoch 2390: 0.020740\n",
      "Cost after epoch 2400: 0.020547\n",
      "Cost after epoch 2410: 0.020455\n",
      "Cost after epoch 2420: 0.020359\n",
      "Cost after epoch 2430: 0.020314\n",
      "Cost after epoch 2440: 0.020225\n",
      "Cost after epoch 2450: 0.020198\n",
      "Cost after epoch 2460: 0.020077\n",
      "Cost after epoch 2470: 0.020000\n",
      "Cost after epoch 2480: 0.019927\n",
      "Cost after epoch 2490: 0.019892\n",
      "Cost after epoch 2500: 0.019847\n",
      "Cost after epoch 2510: 0.019711\n",
      "Cost after epoch 2520: 0.019672\n",
      "Cost after epoch 2530: 0.019664\n",
      "Cost after epoch 2540: 0.019500\n",
      "Cost after epoch 2550: 0.019494\n",
      "Cost after epoch 2560: 0.019435\n",
      "Cost after epoch 2570: 0.019323\n",
      "Cost after epoch 2580: 0.019218\n",
      "Cost after epoch 2590: 0.019212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2600: 0.019090\n",
      "Cost after epoch 2610: 0.019029\n",
      "Cost after epoch 2620: 0.018973\n",
      "Cost after epoch 2630: 0.018928\n",
      "Cost after epoch 2640: 0.018829\n",
      "Cost after epoch 2650: 0.018766\n",
      "Cost after epoch 2660: 0.018692\n",
      "Cost after epoch 2670: 0.018673\n",
      "Cost after epoch 2680: 0.018589\n",
      "Cost after epoch 2690: 0.018524\n",
      "Cost after epoch 2700: 0.018472\n",
      "Cost after epoch 2710: 0.018393\n",
      "Cost after epoch 2720: 0.018429\n",
      "Cost after epoch 2730: 0.018316\n",
      "Cost after epoch 2740: 0.018243\n",
      "Cost after epoch 2750: 0.018164\n",
      "Cost after epoch 2760: 0.018126\n",
      "Cost after epoch 2770: 0.018037\n",
      "Cost after epoch 2780: 0.018015\n",
      "Cost after epoch 2790: 0.017932\n",
      "Cost after epoch 2800: 0.017843\n",
      "Cost after epoch 2810: 0.017834\n",
      "Cost after epoch 2820: 0.017754\n",
      "Cost after epoch 2830: 0.017694\n",
      "Cost after epoch 2840: 0.017707\n",
      "Cost after epoch 2850: 0.017612\n",
      "Cost after epoch 2860: 0.017541\n",
      "Cost after epoch 2870: 0.017518\n",
      "Cost after epoch 2880: 0.017439\n",
      "Cost after epoch 2890: 0.017370\n",
      "Cost after epoch 2900: 0.017390\n",
      "Cost after epoch 2910: 0.017311\n",
      "Cost after epoch 2920: 0.017223\n",
      "Cost after epoch 2930: 0.017219\n",
      "Cost after epoch 2940: 0.017135\n",
      "Cost after epoch 2950: 0.017073\n",
      "Cost after epoch 2960: 0.017089\n",
      "Cost after epoch 2970: 0.016995\n",
      "Cost after epoch 2980: 0.016929\n",
      "Cost after epoch 2990: 0.016867\n",
      "Cost after epoch 3000: 0.016836\n",
      "Cost after epoch 3010: 0.016780\n",
      "Cost after epoch 3020: 0.016720\n",
      "Cost after epoch 3030: 0.016731\n",
      "Cost after epoch 3040: 0.016647\n",
      "Cost after epoch 3050: 0.016581\n",
      "Cost after epoch 3060: 0.016520\n",
      "Cost after epoch 3070: 0.016526\n",
      "Cost after epoch 3080: 0.016451\n",
      "Cost after epoch 3090: 0.016394\n",
      "Cost after epoch 3100: 0.016336\n",
      "Cost after epoch 3110: 0.016297\n",
      "Cost after epoch 3120: 0.016262\n",
      "Cost after epoch 3130: 0.016231\n",
      "Cost after epoch 3140: 0.016162\n",
      "Cost after epoch 3150: 0.016117\n",
      "Cost after epoch 3160: 0.016149\n",
      "Cost after epoch 3170: 0.016025\n",
      "Cost after epoch 3180: 0.016143\n",
      "Cost after epoch 3190: 0.015977\n",
      "Cost after epoch 3200: 0.015932\n",
      "Cost after epoch 3210: 0.015924\n",
      "Cost after epoch 3220: 0.015862\n",
      "Cost after epoch 3230: 0.015830\n",
      "Cost after epoch 3240: 0.015759\n",
      "Cost after epoch 3250: 0.015743\n",
      "Cost after epoch 3260: 0.015697\n",
      "Cost after epoch 3270: 0.015659\n",
      "Cost after epoch 3280: 0.015780\n",
      "Cost after epoch 3290: 0.015632\n",
      "Cost after epoch 3300: 0.015636\n",
      "Cost after epoch 3310: 0.015653\n",
      "Cost after epoch 3320: 0.015607\n",
      "Cost after epoch 3330: 0.015489\n",
      "Cost after epoch 3340: 0.015455\n",
      "Cost after epoch 3350: 0.015455\n",
      "Cost after epoch 3360: 0.015378\n",
      "Cost after epoch 3370: 0.015339\n",
      "Cost after epoch 3380: 0.015307\n",
      "Cost after epoch 3390: 0.015357\n",
      "Cost after epoch 3400: 0.015283\n",
      "Cost after epoch 3410: 0.015256\n",
      "Cost after epoch 3420: 0.015215\n",
      "Cost after epoch 3430: 0.015151\n",
      "Cost after epoch 3440: 0.015133\n",
      "Cost after epoch 3450: 0.015073\n",
      "Cost after epoch 3460: 0.015086\n",
      "Cost after epoch 3470: 0.015046\n",
      "Cost after epoch 3480: 0.015032\n",
      "Cost after epoch 3490: 0.014971\n",
      "Cost after epoch 3500: 0.014965\n",
      "Cost after epoch 3510: 0.014920\n",
      "Cost after epoch 3520: 0.014911\n",
      "Cost after epoch 3530: 0.014878\n",
      "Cost after epoch 3540: 0.014861\n",
      "Cost after epoch 3550: 0.014837\n",
      "Cost after epoch 3560: 0.014809\n",
      "Cost after epoch 3570: 0.014772\n",
      "Cost after epoch 3580: 0.014724\n",
      "Cost after epoch 3590: 0.014769\n",
      "Cost after epoch 3600: 0.014702\n",
      "Cost after epoch 3610: 0.014691\n",
      "Cost after epoch 3620: 0.014659\n",
      "Cost after epoch 3630: 0.014618\n",
      "Cost after epoch 3640: 0.014585\n",
      "Cost after epoch 3650: 0.014554\n",
      "Cost after epoch 3660: 0.014541\n",
      "Cost after epoch 3670: 0.014507\n",
      "Cost after epoch 3680: 0.014491\n",
      "Cost after epoch 3690: 0.014494\n",
      "Cost after epoch 3700: 0.014484\n",
      "Cost after epoch 3710: 0.014462\n",
      "Cost after epoch 3720: 0.014409\n",
      "Cost after epoch 3730: 0.014426\n",
      "Cost after epoch 3740: 0.014387\n",
      "Cost after epoch 3750: 0.014326\n",
      "Cost after epoch 3760: 0.014532\n",
      "Cost after epoch 3770: 0.014365\n",
      "Cost after epoch 3780: 0.014279\n",
      "Cost after epoch 3790: 0.014266\n",
      "Cost after epoch 3800: 0.014265\n",
      "Cost after epoch 3810: 0.014232\n",
      "Cost after epoch 3820: 0.014189\n",
      "Cost after epoch 3830: 0.014217\n",
      "Cost after epoch 3840: 0.014182\n",
      "Cost after epoch 3850: 0.014144\n",
      "Cost after epoch 3860: 0.014122\n",
      "Cost after epoch 3870: 0.014099\n",
      "Cost after epoch 3880: 0.014068\n",
      "Cost after epoch 3890: 0.014066\n",
      "Cost after epoch 3900: 0.014059\n",
      "Cost after epoch 3910: 0.014026\n",
      "Cost after epoch 3920: 0.014006\n",
      "Cost after epoch 3930: 0.014025\n",
      "Cost after epoch 3940: 0.013981\n",
      "Cost after epoch 3950: 0.013953\n",
      "Cost after epoch 3960: 0.013952\n",
      "Cost after epoch 3970: 0.013972\n",
      "Cost after epoch 3980: 0.013921\n",
      "Cost after epoch 3990: 0.013933\n",
      "Cost after epoch 4000: 0.013894\n",
      "Cost after epoch 4010: 0.013868\n",
      "Cost after epoch 4020: 0.013826\n",
      "Cost after epoch 4030: 0.013895\n",
      "Cost after epoch 4040: 0.013859\n",
      "Cost after epoch 4050: 0.013800\n",
      "Cost after epoch 4060: 0.013763\n",
      "Cost after epoch 4070: 0.013755\n",
      "Cost after epoch 4080: 0.013734\n",
      "Cost after epoch 4090: 0.013710\n",
      "Cost after epoch 4100: 0.013701\n",
      "Cost after epoch 4110: 0.013673\n",
      "Cost after epoch 4120: 0.013654\n",
      "Cost after epoch 4130: 0.013640\n",
      "Cost after epoch 4140: 0.013638\n",
      "Cost after epoch 4150: 0.013621\n",
      "Cost after epoch 4160: 0.013592\n",
      "Cost after epoch 4170: 0.013594\n",
      "Cost after epoch 4180: 0.013585\n",
      "Cost after epoch 4190: 0.013558\n",
      "Cost after epoch 4200: 0.013531\n",
      "Cost after epoch 4210: 0.013519\n",
      "Cost after epoch 4220: 0.013515\n",
      "Cost after epoch 4230: 0.013493\n",
      "Cost after epoch 4240: 0.013471\n",
      "Cost after epoch 4250: 0.013522\n",
      "Cost after epoch 4260: 0.013474\n",
      "Cost after epoch 4270: 0.013446\n",
      "Cost after epoch 4280: 0.013418\n",
      "Cost after epoch 4290: 0.013487\n",
      "Cost after epoch 4300: 0.013418\n",
      "Cost after epoch 4310: 0.013382\n",
      "Cost after epoch 4320: 0.013367\n",
      "Cost after epoch 4330: 0.013348\n",
      "Cost after epoch 4340: 0.013343\n",
      "Cost after epoch 4350: 0.013329\n",
      "Cost after epoch 4360: 0.013305\n",
      "Cost after epoch 4370: 0.013290\n",
      "Cost after epoch 4380: 0.013287\n",
      "Cost after epoch 4390: 0.013299\n",
      "Cost after epoch 4400: 0.013272\n",
      "Cost after epoch 4410: 0.013237\n",
      "Cost after epoch 4420: 0.013223\n",
      "Cost after epoch 4430: 0.013273\n",
      "Cost after epoch 4440: 0.013217\n",
      "Cost after epoch 4450: 0.013188\n",
      "Cost after epoch 4460: 0.013171\n",
      "Cost after epoch 4470: 0.013214\n",
      "Cost after epoch 4480: 0.013177\n",
      "Cost after epoch 4490: 0.013143\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochsTrain = 4500 #9500 #4500 #7200\n",
    "layers_train = [13, 10, 5, 10, 5, 10, 1]\n",
    "parameters, costs = model(train_X, train_Y, learning_rate = 0.001, layers=layers_train , num_epochs = num_epochsTrain)\n",
    "plt.plot(np.arange(num_epochsTrain)+1, costs)\n",
    "plt.xlabel(\"Iterations(m)\")\n",
    "plt.ylabel(\"Cost function(J)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6a13a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 91)\n",
      "(1, 91)\n"
     ]
    }
   ],
   "source": [
    "df_testing = pd.read_csv(\"./Dataset/TestingDataset.csv\")\n",
    "for column in columnsToNormalize:\n",
    "    df_testing[column] = MinMaxScaler().fit_transform(np.array(df_testing[column]).reshape(-1,1))\n",
    "\n",
    "test_Y = np.array(df_testing['target'])\n",
    "df_testing.drop(['target'], axis=1, inplace=True)\n",
    "test_Y = test_Y.astype('float32')\n",
    "test_Y = test_Y.reshape(test_Y.shape[0],1)\n",
    "test_X = np.array(df_testing.values)\n",
    "test_X = test_X.astype('float32')\n",
    "test_X = tf.transpose(test_X)\n",
    "test_Y = tf.transpose(test_Y)\n",
    "\n",
    "df_testing.head()\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "57905ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the testing Dataset was of 96%\n",
      "The accuracy on the train Dataset was of 99% \n"
     ]
    }
   ],
   "source": [
    "Z3 = forward_propagation(test_X, parameters, layers=layers_train)\n",
    "A3 = tf.keras.activations.sigmoid(Z3)\n",
    "A3 = tf.math.round(A3)\n",
    "# print(A3)\n",
    "trueVector = A3 == test_Y\n",
    "goodPredictionsCounter = trueVector.numpy().sum()\n",
    "perc = (goodPredictionsCounter/test_Y.shape[1]) * 100\n",
    "print(\"The accuracy on the testing Dataset was of %i%%\" % (perc))\n",
    "\n",
    "Z3 = forward_propagation(train_X, parameters, layers=layers_train)\n",
    "A3 = tf.keras.activations.sigmoid(Z3)\n",
    "A3 = tf.math.round(A3)\n",
    "# print(A3)\n",
    "trueVector = A3 == train_Y\n",
    "goodPredictionsCounter = trueVector.numpy().sum()\n",
    "perc = (goodPredictionsCounter/train_Y.shape[1]) * 100\n",
    "print(\"The accuracy on the train Dataset was of %i%% \" % (perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b7e34835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model at the momment\n",
    "best_model = parameters\n",
    "best_layers = layers_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fed22",
   "metadata": {},
   "source": [
    "# The best performance reached in my experiment with differents hyperparameters was of 99% of accuracy on the Train Dataset and 96% of accuracy on the Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0390684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
