{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1acc0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf262d3",
   "metadata": {},
   "source": [
    "Load and pre-process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1463d220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart = pd.read_csv(\"./Dataset/HeartDataset.csv\")\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ff3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df_heart.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12da5c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333    1   3  0.481132  0.244292    1        0  0.603053      0   \n",
       "1  0.166667    1   2  0.339623  0.283105    0        1  0.885496      0   \n",
       "2  0.250000    0   1  0.339623  0.178082    0        0  0.770992      0   \n",
       "3  0.562500    1   1  0.245283  0.251142    0        1  0.816794      0   \n",
       "4  0.583333    0   0  0.245283  0.520548    0        1  0.702290      1   \n",
       "\n",
       "   oldpeak  slope  ca  thal  target  \n",
       "0      2.3      0   0     1       1  \n",
       "1      3.5      0   0     2       1  \n",
       "2      1.4      2   0     2       1  \n",
       "3      0.8      2   0     2       1  \n",
       "4      0.6      2   0     2       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the Dataset using sklearn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columnsToNormalize = ['age', 'trestbps', 'chol', 'thalach']\n",
    "for column in columnsToNormalize:\n",
    "    df_normalized[column] = MinMaxScaler().fit_transform(np.array(df_normalized[column]).reshape(-1,1))\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac7d8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(299, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract the target column as our true label vector\n",
    "train_Y = np.array(df_heart['target'])\n",
    "# df_normalized.drop(['target'], axis=1, inplace=True)\n",
    "train_Y = train_Y.astype('float32')\n",
    "train_Y = train_Y.reshape(299,1)\n",
    "print(train_Y)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b74b8e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 13)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(df_normalized.values)\n",
    "train_X = train_X.astype('float32')\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f1e10",
   "metadata": {},
   "source": [
    "Now out Dataset and X,Y train variables are ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5805cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initialization for our parameters W,b\n",
    "\n",
    "def initialize_parameters():\n",
    "    # My model will have tree layers of 13 (Features layer), 5 Units, 3 Units and the Output Layer with 1 Unit\n",
    "    # I will use Tensorflow Keras initializers to do this\n",
    "    \n",
    "    # And... I'll use the He initialization because i have Relu activations in my model\n",
    "    initializerHe = tf.keras.initializers.HeNormal()\n",
    "    \n",
    "    # And a Glotot Normal to the output layer (Sigmoid)\n",
    "    initializerGn = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    W1 = tf.Variable(initializerHe(shape=(5, 13)))\n",
    "    b1 = tf.Variable(initializerHe(shape=(5, 1)))\n",
    "    W2 = tf.Variable(initializerHe(shape=(3, 5)))\n",
    "    b2 = tf.Variable(initializerHe(shape=(3, 1)))\n",
    "    W3 = tf.Variable(initializerGn(shape=(1, 3)))\n",
    "    b3 = tf.Variable(initializerGn(shape=(1, 1)))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a06371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d2423",
   "metadata": {},
   "source": [
    "Now, create the Fordward Propagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e1fc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve the parameters\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # Activations...\n",
    "    \n",
    "    Z1 = tf.math.add(tf.linalg.matmul(W1,X), b1)\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.math.add(tf.linalg.matmul(W2,A1), b2)\n",
    "    A2 = tf.keras.activations.relu(Z2)\n",
    "    Z3 = tf.math.add(tf.linalg.matmul(W3,A2), b3)\n",
    "    A3 = tf.keras.activations.sigmoid(Z3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8a69bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2.8020773   2.425813    2.4129038   2.3041353   5.3234973   4.206979\n",
      "   2.8335092   2.7965977   2.7910306   2.2405286   2.078617    2.799202\n",
      "   2.1029239   4.497656    2.8106759   2.8082342   2.799555    2.781363\n",
      "   0.70624137  2.4388452   2.815924    3.6328304   1.1668595   2.8724022\n",
      "   2.8019772   1.319076    1.5031724   2.7868295   2.244443    2.808623\n",
      "   1.981213    2.0015996   2.0861108   2.7973244   3.6472511   2.8309538\n",
      "   2.8006988   2.7864017   2.802499    2.8244362   2.4891822   2.8163092\n",
      "   1.0513648   2.8391142   2.7994015   2.531636    2.800053    2.7991192\n",
      "   4.9805064   2.1648984   2.8171282   3.3843002   0.6088457   2.8018563\n",
      "   2.81411     0.81560636  1.0239623   1.009147    2.7852712   1.1748395\n",
      "   2.5940015   2.7986374   2.7838345   2.7663255   2.590956    1.4273067\n",
      "   4.749012    2.8321936   2.4618056   2.6636648   2.799543    2.8052118\n",
      "   1.7182647   1.8149478   2.796392    2.8333154   1.7755171   6.3951235\n",
      "   1.5050068   2.824621    2.7845955   2.262187    2.7944565   2.7765894\n",
      "   2.8374774   2.812894    2.5801597   1.9801934   2.8080325   2.8362339\n",
      "   0.11570048  2.9065428  -1.3766356   1.3023767   2.8129652   1.9712133\n",
      "   3.098279   -2.8164268   0.8433279  -1.4790001   1.5419905   2.796538\n",
      "   1.1668214   2.787948    2.783749    2.8189383   2.78542     2.8400536\n",
      "   3.2492943   2.108973    6.0460463   1.9919758   2.7964563   1.7705328\n",
      "   2.6945841   2.7956192   2.5159814   2.793048    2.8100386   2.8400536\n",
      "   0.9158391   1.7060207   2.8353684   2.813573    2.7972186   2.7594705\n",
      "   1.2176479   2.793605    2.808793    4.563884    2.7904248   2.8123558\n",
      "   2.5427408   2.4223554   2.8083537   2.3587728   2.8011613   1.4035326\n",
      "   4.750859    3.292877    2.8188677   1.6388651   2.794898    0.11733007\n",
      "   2.7804983   3.350354    2.7973683   2.7880495   2.7840922   2.7833264\n",
      "   1.4559826   3.872683    2.7779493   2.8108182   2.7937393   3.292212\n",
      "   2.7838314   2.2184505  -0.7873213   2.8122456   2.794314    4.5058393\n",
      "   2.2854638  -0.7619581   0.24226141  0.97759295  1.4354742   1.9102058\n",
      "   1.6148587   2.801882    2.1829515  -0.09480286  0.24974322  2.0384102\n",
      "  -1.4194741   2.7820947   1.8342017   2.857415    1.8185512   0.36123085\n",
      "   2.5503716   2.0389724   1.5799582  -0.1425693   0.76624465  1.2565542\n",
      "   2.7818594   1.2985868   2.996166   -0.78916526  1.1517675   0.12865949\n",
      "   2.1267905   2.5833993   1.6064379  -0.05301142  0.5313473  -1.4979486\n",
      "  -0.3127792   1.0360968   2.3594491   2.8232255  -0.84744906  0.83901715\n",
      "   1.834556    1.1340036   0.24750304  0.88959754  2.7972715   0.54926896\n",
      "   2.0903375   3.096024    0.38273048  2.4716947   2.7993972  -2.516028\n",
      "   0.7136531  -1.4631386  -0.28516388  1.1609681   2.5263205   1.0450877\n",
      "   0.8448725   2.8156664   2.033472    2.1573987   2.7739882   4.7394304\n",
      "   2.783875   -1.6336536   1.9377332   1.5975009  -1.4466338   2.1379442\n",
      "  -0.5007312   0.4907534  -1.4278054   1.5793474   2.9958117   4.6883917\n",
      "  -1.0256054   1.7462636   0.32651043  2.6913645   1.6958278   1.7763393\n",
      "   1.7810044   0.57180834 -1.7406316  -2.824833   -1.1116424   0.5309844\n",
      "   2.7811286  -0.52117825 -0.04287648  2.7986548   4.7422643   2.8034568\n",
      "   0.9782177   0.04246807  0.32952905  1.2328246   1.452198    0.9553819\n",
      "   4.5325766  -1.0115232  -0.77444506  2.7604418   1.3700293   1.2463382\n",
      "   4.495792    0.73877954  1.1861864  -0.9060445   0.9244932   2.7995687\n",
      "  -0.00959015 -0.04696631  3.873072    1.481997    0.31740856  1.7601099\n",
      "   0.54107285  0.40805602  1.8566871   1.1179491   0.8793274   2.5487776\n",
      "   0.87503886 -2.5320463  -0.6117089   2.8014927   1.7200549  -1.3900237\n",
      "   4.694247   -1.5150218   2.8400536   2.771523    2.8225746 ]], shape=(1, 299), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Fordward Propagation test\n",
    "print(forward_propagation(train_X.transpose(), parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b79451",
   "metadata": {},
   "source": [
    "Let's create the Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "837265bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(predicted_Y, true_Y):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(true_Y, predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60e42b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.923182>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cost testing\n",
    "pred = tf.constant([[ 2.4048107,   5.0334096 ],\n",
    "             [-0.7921977,  -4.1523376 ],\n",
    "             [ 0.9447198,  -0.46802214],\n",
    "             [ 1.158121,    3.9810789 ],\n",
    "             [ 4.768706,    2.3220146 ],\n",
    "             [ 6.1481323,   3.909829  ]])\n",
    "true = tf.constant([[ 1,   0 ],\n",
    "             [1,  1 ],\n",
    "             [ 0,  0],\n",
    "             [ 0,    0 ],\n",
    "             [ 1,    1 ],\n",
    "             [ 1,   0  ]])\n",
    "compute_cost(pred, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae7db8",
   "metadata": {},
   "source": [
    "Now, let's create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f15c0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, learning_rate = 0.0001, num_epochs = 1500, print_cost = True):\n",
    "    \n",
    "    # Save all the costs around the training\n",
    "    costs = []\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # I want to use the Adam optimizer in my model, in my experience, it's very efficient\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Do the fordward propagation\n",
    "            Z3 = forward_propagation(tf.transpose(X_train), parameters)\n",
    "\n",
    "            # Compute the cost function\n",
    "            cost = compute_cost(Z3, tf.transpose(Y_train))\n",
    "\n",
    "        trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "        grads = tape.gradient(cost, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f0c89e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.676989\n",
      "Cost after epoch 10: 0.654139\n",
      "Cost after epoch 20: 0.634664\n",
      "Cost after epoch 30: 0.618241\n",
      "Cost after epoch 40: 0.604630\n",
      "Cost after epoch 50: 0.592816\n",
      "Cost after epoch 60: 0.581838\n",
      "Cost after epoch 70: 0.572191\n",
      "Cost after epoch 80: 0.563883\n",
      "Cost after epoch 90: 0.556366\n",
      "Cost after epoch 100: 0.549535\n",
      "Cost after epoch 110: 0.543200\n",
      "Cost after epoch 120: 0.537139\n",
      "Cost after epoch 130: 0.531416\n",
      "Cost after epoch 140: 0.526046\n",
      "Cost after epoch 150: 0.520884\n",
      "Cost after epoch 160: 0.515969\n",
      "Cost after epoch 170: 0.511405\n",
      "Cost after epoch 180: 0.507231\n",
      "Cost after epoch 190: 0.503513\n",
      "Cost after epoch 200: 0.499891\n",
      "Cost after epoch 210: 0.496449\n",
      "Cost after epoch 220: 0.493146\n",
      "Cost after epoch 230: 0.490008\n",
      "Cost after epoch 240: 0.486981\n",
      "Cost after epoch 250: 0.484050\n",
      "Cost after epoch 260: 0.481295\n",
      "Cost after epoch 270: 0.478645\n",
      "Cost after epoch 280: 0.476115\n",
      "Cost after epoch 290: 0.473679\n",
      "Cost after epoch 300: 0.471204\n",
      "Cost after epoch 310: 0.468589\n",
      "Cost after epoch 320: 0.465838\n",
      "Cost after epoch 330: 0.462965\n",
      "Cost after epoch 340: 0.459909\n",
      "Cost after epoch 350: 0.456877\n",
      "Cost after epoch 360: 0.453884\n",
      "Cost after epoch 370: 0.451050\n",
      "Cost after epoch 380: 0.448588\n",
      "Cost after epoch 390: 0.445950\n",
      "Cost after epoch 400: 0.443242\n",
      "Cost after epoch 410: 0.440570\n",
      "Cost after epoch 420: 0.437953\n",
      "Cost after epoch 430: 0.435617\n",
      "Cost after epoch 440: 0.433425\n",
      "Cost after epoch 450: 0.431173\n",
      "Cost after epoch 460: 0.428626\n",
      "Cost after epoch 470: 0.425246\n",
      "Cost after epoch 480: 0.421581\n",
      "Cost after epoch 490: 0.418203\n",
      "Cost after epoch 500: 0.415253\n",
      "Cost after epoch 510: 0.412325\n",
      "Cost after epoch 520: 0.409544\n",
      "Cost after epoch 530: 0.406879\n",
      "Cost after epoch 540: 0.404259\n",
      "Cost after epoch 550: 0.401663\n",
      "Cost after epoch 560: 0.399139\n",
      "Cost after epoch 570: 0.396839\n",
      "Cost after epoch 580: 0.394564\n",
      "Cost after epoch 590: 0.392328\n",
      "Cost after epoch 600: 0.390214\n",
      "Cost after epoch 610: 0.388044\n",
      "Cost after epoch 620: 0.385979\n",
      "Cost after epoch 630: 0.384169\n",
      "Cost after epoch 640: 0.382373\n",
      "Cost after epoch 650: 0.380511\n",
      "Cost after epoch 660: 0.378637\n",
      "Cost after epoch 670: 0.376878\n",
      "Cost after epoch 680: 0.375211\n",
      "Cost after epoch 690: 0.373603\n",
      "Cost after epoch 700: 0.372049\n",
      "Cost after epoch 710: 0.370572\n",
      "Cost after epoch 720: 0.369178\n",
      "Cost after epoch 730: 0.367843\n",
      "Cost after epoch 740: 0.366537\n",
      "Cost after epoch 750: 0.365169\n",
      "Cost after epoch 760: 0.363815\n",
      "Cost after epoch 770: 0.362508\n",
      "Cost after epoch 780: 0.361285\n",
      "Cost after epoch 790: 0.360100\n",
      "Cost after epoch 800: 0.358959\n",
      "Cost after epoch 810: 0.357859\n",
      "Cost after epoch 820: 0.356783\n",
      "Cost after epoch 830: 0.355715\n",
      "Cost after epoch 840: 0.354660\n",
      "Cost after epoch 850: 0.353642\n",
      "Cost after epoch 860: 0.352635\n",
      "Cost after epoch 870: 0.351654\n",
      "Cost after epoch 880: 0.350695\n",
      "Cost after epoch 890: 0.349754\n",
      "Cost after epoch 900: 0.348837\n",
      "Cost after epoch 910: 0.347932\n",
      "Cost after epoch 920: 0.347043\n",
      "Cost after epoch 930: 0.346173\n",
      "Cost after epoch 940: 0.345328\n",
      "Cost after epoch 950: 0.344509\n",
      "Cost after epoch 960: 0.343702\n",
      "Cost after epoch 970: 0.342912\n",
      "Cost after epoch 980: 0.342148\n",
      "Cost after epoch 990: 0.341397\n",
      "Cost after epoch 1000: 0.340656\n",
      "Cost after epoch 1010: 0.339923\n",
      "Cost after epoch 1020: 0.339201\n",
      "Cost after epoch 1030: 0.338492\n",
      "Cost after epoch 1040: 0.337792\n",
      "Cost after epoch 1050: 0.337101\n",
      "Cost after epoch 1060: 0.336419\n",
      "Cost after epoch 1070: 0.335743\n",
      "Cost after epoch 1080: 0.335066\n",
      "Cost after epoch 1090: 0.334387\n",
      "Cost after epoch 1100: 0.333724\n",
      "Cost after epoch 1110: 0.333079\n",
      "Cost after epoch 1120: 0.332443\n",
      "Cost after epoch 1130: 0.331815\n",
      "Cost after epoch 1140: 0.331196\n",
      "Cost after epoch 1150: 0.330587\n",
      "Cost after epoch 1160: 0.329990\n",
      "Cost after epoch 1170: 0.329400\n",
      "Cost after epoch 1180: 0.328828\n",
      "Cost after epoch 1190: 0.328265\n",
      "Cost after epoch 1200: 0.327712\n",
      "Cost after epoch 1210: 0.327168\n",
      "Cost after epoch 1220: 0.326631\n",
      "Cost after epoch 1230: 0.326098\n",
      "Cost after epoch 1240: 0.325572\n",
      "Cost after epoch 1250: 0.325045\n",
      "Cost after epoch 1260: 0.324528\n",
      "Cost after epoch 1270: 0.323993\n",
      "Cost after epoch 1280: 0.323474\n",
      "Cost after epoch 1290: 0.322960\n",
      "Cost after epoch 1300: 0.322448\n",
      "Cost after epoch 1310: 0.321943\n",
      "Cost after epoch 1320: 0.321445\n",
      "Cost after epoch 1330: 0.320955\n",
      "Cost after epoch 1340: 0.320453\n",
      "Cost after epoch 1350: 0.319939\n",
      "Cost after epoch 1360: 0.319425\n",
      "Cost after epoch 1370: 0.318871\n",
      "Cost after epoch 1380: 0.318318\n",
      "Cost after epoch 1390: 0.317790\n",
      "Cost after epoch 1400: 0.317268\n",
      "Cost after epoch 1410: 0.316755\n",
      "Cost after epoch 1420: 0.316251\n",
      "Cost after epoch 1430: 0.315758\n",
      "Cost after epoch 1440: 0.315266\n",
      "Cost after epoch 1450: 0.314785\n",
      "Cost after epoch 1460: 0.314321\n",
      "Cost after epoch 1470: 0.313868\n",
      "Cost after epoch 1480: 0.313428\n",
      "Cost after epoch 1490: 0.312998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochsTrain = 1500\n",
    "parameters, costs = model(train_X, train_Y, learning_rate = 0.0005,num_epochs = num_epochsTrain)\n",
    "plt.plot(np.arange(num_epochsTrain)+1, costs)\n",
    "plt.xlabel(\"Iterations(m)\")\n",
    "plt.ylabel(\"Cost function(J)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fe8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
